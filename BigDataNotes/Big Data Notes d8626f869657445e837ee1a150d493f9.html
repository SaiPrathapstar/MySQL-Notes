<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Big Data Notes</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
    margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-gray_background {
	background: rgba(241, 241, 239, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.highlight-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-gray_background {
	background: rgba(241, 241, 239, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.block-color-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-uiBlue { background-color: rgba(35, 131, 226, .07); }
.select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
.select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
.select-value-color-green { background-color: rgba(219, 237, 219, 1); }
.select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
.select-value-color-translucentGray { background-color: rgba(255, 255, 255, 0.0375); }
.select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
.select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
.select-value-color-red { background-color: rgba(255, 226, 221, 1); }
.select-value-color-yellow { background-color: rgba(253, 236, 200, 1); }
.select-value-color-blue { background-color: rgba(211, 229, 239, 1); }
.select-value-color-pageGlass { background-color: undefined; }
.select-value-color-washGlass { background-color: undefined; }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="d8626f86-9657-445e-837e-e1a150d493f9" class="page sans"><header><h1 class="page-title">Big Data Notes</h1><p class="page-description"></p></header><div class="page-body"><h3 id="27402ec9-9df0-46f8-9f7a-221196707368" class=""><strong>Big Data</strong></h3><ul id="52cf23f6-f753-4cbf-99d7-675f0d4630ce" class="bulleted-list"><li style="list-style-type:disc">RDMS is only for limited volume of data.</li></ul><ul id="4d1dbdd7-2a87-4497-aefb-af2213a9ff00" class="bulleted-list"><li style="list-style-type:disc">Big data is a concept (not a software or hardware). To apply this there are two frameworks are there they are i) Hadoop and ii) Spark.</li></ul><p id="565b7e3b-5d22-4c61-9594-7a9f4b96ef92" class="">Big data means:</p><ol type="1" id="3d9f8537-c60f-433a-a3a1-2c28afb8df87" class="numbered-list" start="1"><li>Variety: Structured, semi-structured and unstructured</li></ol><ol type="1" id="c3fbe7ad-9661-4b2f-9f67-1168a7606cb0" class="numbered-list" start="2"><li>Volume: The size and amounts of data</li></ol><ol type="1" id="b3853831-3f23-4aea-a8f6-ec3fcf2933df" class="numbered-list" start="3"><li>Velocity: The speed at companies receive data</li></ol><ol type="1" id="bbcf8d4e-0cef-4069-8b10-76c06856c6cc" class="numbered-list" start="4"><li>Veracity: Trust worthiness or accuracy of data</li></ol><ol type="1" id="78a8f0f3-0792-49fe-8537-2d24a6c5fb4f" class="numbered-list" start="5"><li>Value: The benefits that the big data can provide</li></ol><ul id="cb19ea48-8473-44eb-a0a8-1f503ad716e3" class="bulleted-list"><li style="list-style-type:disc">Hadoop is used to analyse in Disk (Hard disk). Spark is used to analyse in Memory(RAM).</li></ul><ul id="10101b49-65a0-4776-a82b-5fdc9381cdbb" class="bulleted-list"><li style="list-style-type:disc">Spark is much faster than Hadoop. But both produce the same result.</li></ul><ul id="1c547405-acac-43fe-ab22-302b9061eaff" class="bulleted-list"><li style="list-style-type:disc">Spark is very expensive. Hadoop and Spark both are owned by Apache. They are open source.</li></ul><ul id="9382b4c2-6b43-43d3-8560-6ce1b50e878d" class="bulleted-list"><li style="list-style-type:disc">MySQL, Oracle, etc., have storage in themselves, they store the data inside them only. Hadoops and Spark work as engine with the data. The data itself is stored in DFS (Distributed file system).</li></ul><ul id="5c99c222-981c-498a-bf41-5adafa120716" class="bulleted-list"><li style="list-style-type:disc">In DFS the data is divided into smaller chunks and stored it in multiple systems.</li></ul><ul id="74bc8e16-e39c-465d-84ac-06a6539ab195" class="bulleted-list"><li style="list-style-type:disc">DFS is not a database or a data warehouse, because it does not have any engines, APIs connected to it. it is just an offline flat file system.</li></ul><ul id="47860f09-0562-46ba-8b8c-1a6271f9c19a" class="bulleted-list"><li style="list-style-type:disc">In Hadoop, we don’t use a database at all. We use DFS, Data warehouse (modern), Data lake, Delta lake, Data lake house.</li></ul><ul id="e43566e7-346e-449e-beda-c94c9fa5940c" class="bulleted-list"><li style="list-style-type:disc">The output is also stored in the same DFS; we don’t need other place.</li></ul><p id="1e4dafd4-efab-48d8-acb4-1c3638ac6d9f" class=""><strong>Hadoop</strong></p><figure id="ed02e8ce-b670-4472-9d77-a48dbc5ea7ad" class="image"><a href="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image1.jpeg"><img style="width:700px" src="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image1.jpeg"/></a></figure><p id="ad54ee90-66ee-45a9-b443-f09734476cfc" class="">It is a software.</p><p id="07906517-a9c9-4db7-925c-cf4fda8c3832" class="">The engine in Hadoop is MapReduce, it is the first distributed programming language.</p><p id="bbc99744-18a8-40d0-a474-030606f5ed45" class="">Hadoop = HDFS + MapReduce. MapReduce is used to process and HDFS is used for storage.</p><p id="7e468473-be53-4c80-b045-fdcf26b8190f" class="">In Hadoop cluster, HDFS knows only MapReduce language. Even if you write code in SQL it is converted into MapReduce.</p><p id="400ea821-9ae5-409f-9397-7ba74fa8a991" class=""><strong>SQOOP</strong> is used to bring data from traditional data sources like MySQL into DFS. This tool is available in Hadoop itself. This process is called ETL. SQOOP is also used to export the data.</p><p id="9d0a7032-64b7-48d1-a3a6-a210eb44ad1f" class="">SQOOP can be used only for only static sources.</p><p id="01bec4b9-92e3-4143-8737-90b6ba90e84e" class="">To get data from dynamic sources we need to use <strong>FLUME</strong>. We cannot do exporting using FLUME.</p><p id="46ef1ad4-0380-4e17-b98d-897452e8afe6" class="">The output should always be in structured format only; the input format doesn’t matter.</p><p id="60d452c5-9aac-4f6d-a2b2-63b0b4e2ddb2" class="">Importing data is called Data ingestion.</p><p id="cde31cff-51b8-4a54-a55d-7c0086cf6ae6" class="">Pipeline: Connection from source to target to get the data.</p><p id="802191e8-789b-4cb1-ba0c-8521c8957e35" class="">SQOOP and FLUME are not used for any kind of analysis. It is just used for data ingestion.</p><p id="259d8bd4-b701-4809-af3f-94a2e2bd053b" class="">We can do processing and analysis in Hadoop itself. In order to analyze we need to use MapReduce language. MapReduce code is very big so, <strong>HIVE</strong> is created. The hive code is then converted into MapReduce. Hive code is similar to SQL. Hive is a Data Warehouse; it uses HDFS as storage. It is dependent on HDFS, so it is not independent.</p><p id="31554079-aad4-46e3-939d-59d58601a261" class="">We can also apply transformations using Hadoop, but only some are possible.</p><p id="42c9ddbd-7f43-41d3-a8dc-aece2cbc0db3" class="">Pig is also used for analysis language. But it is not currently used.</p><p id="dd3fb898-3581-4ec9-b99b-35de3ca22e50" class="">All these languages (MRP, Hive, Pig) are used for analysis called Batch Processing (offline processing) and HBase (NoSQL, online processing, OLAP). HBase is not converted into MapReduce, it is directly connected to HDFS with the help of Zookeeper.</p><p id="a40b6c11-8598-47f3-8cc6-3c7a5d4f2cbc" class="">If any request is going through MapReduce it is offline processing. HBase is on top of HDFS (It doesn’t need MapReduce).</p><p id="6289fa23-252f-4537-8078-9522714e7c9b" class="">Online processing: If you want to filter records based on a column, offline processing reads only that column and not the entire file or table.</p><p id="51e8d456-3491-44de-b17d-12f0d7fa6771" class="">Offline processing: It scans the entire file (like SQL).</p><p id="7d4ff137-4ec1-44b4-b8da-ea018aee0b56" class="">Zoo keeper manages all these components (SQOOP, FLUME, MapReduce, HDFS, HBase) and cluster management (like name node, etc.,).</p><p id="30aaac1e-78f9-45d7-9a49-07a86433f785" class="">Oozie is the component of Hadoop eco system that is used to schedule the jobs.</p><p id="f648d8d0-2cf3-48fb-9f6d-2c3ec776ee8f" class="">Drawbacks of Hadoop 1 cluster</p><ol type="1" id="c10636d2-0599-4ace-b354-6f9d188f150c" class="numbered-list" start="1"><li>No resource management (It follows Queue structure, one process after other, no parallel working)</li></ol><ol type="1" id="4ad300d3-19f3-47c2-9716-29459701c260" class="numbered-list" start="2"><li>No High availability (only 1 master machine)</li></ol><p id="a7abb0e1-5681-47b5-84eb-41157cb17e0a" class="">To overcome these problems, Hadoop 2 has been introduced. In this version, YARN (yet another resource negotiator) has been introduced as a replacement of MapReduce. There are two master machines (high configuration system) which always run in parallel. YARN performs all the jobs parallelly.</p><p id="aa0ca2a2-9270-4cc3-8a4e-0389d743cdbf" class="">Note: We cannot modify the data when the data in HDFS because it is a file system and not a database.</p><p id="b47606a1-c2f2-4ce9-930a-fc82e1a9dd78" class=""><strong>HDFS</strong></p><p id="6fbb6f4a-a516-493c-93d8-ad3b59b2d84c" class="">Cluster -&gt; Data Centers -&gt; Racks -&gt; Nodes</p><p id="0c04a8dd-630e-4704-9632-e56c52dab0b5" class="">Name node is the master node and it is also called meta node.</p><p id="21229a53-7322-4cdc-be0f-65f4636ce9ba" class="">Within 3 seconds if the slave doesn’t respond then the slave is treated as dead node.</p><p id="c1087394-6595-4bd4-afee-96b0939390df" class="">Name node is the master node. Data nodes are slave nodes.</p><p id="9c34f6af-3ced-48a8-aecc-ee7a421c7460" class="">Min 50GB storage for each data node.</p><p id="d8532191-aeb2-4d78-879c-f2b35c920df9" class="">HDFS default replication size is 3</p><p id="43a833d4-6e68-4651-b9f8-ce4156d0d2a5" class="">DFS is also called as Data lake.</p><p id="8dddf848-ad32-4d4a-964e-d4ac8b519628" class="">Default block size is 128MB recommended</p><figure id="bb1aae11-0eee-4c77-9a46-0bbc60576d21" class="image"><a href="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image2.jpeg"><img style="width:700px" src="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image2.jpeg"/></a></figure><p id="dbdece7f-89e0-4a8e-ad51-281452b0f425" class="">We can check the HDFS files in link: <a href="http://localhost:50070/">http://localhost:50070</a></p><p id="33292018-dabf-4319-ab63-c198d71dc7c2" class="">We can check the currently running tasks in MapReduce only. HDFS does not store the details of it Link for MapReduce: <a href="http://localhost:50030/">http://localhost:50030</a></p><p id="fb384f6f-0ec0-46b4-9df8-95563c44c680" class="">DFS in cloud:</p><ol type="1" id="58526ea8-a73e-4061-86a6-52f95c1857f0" class="numbered-list" start="1"><li>In AWS: S3</li></ol><ol type="1" id="323b890d-32d0-403c-9a0c-828bc9fc28ea" class="numbered-list" start="2"><li>Azure: ADLS (Azure data lake Service)</li></ol><ol type="1" id="dc18d4ac-b60a-40e7-9120-b8f8e626088a" class="numbered-list" start="3"><li>Google cloud storage</li></ol><p id="8101e8ec-9b21-4b0a-9f6d-f77517a39b76" class="">The Name nodes has two files associated with it, they are FSImage and EditLog. The FS Image has the information about complete state of the file system since the start of the Hadoop. The edit log contains all the recent modifications made to the file system.</p><p id="f788e517-28fc-4039-84d9-4e611a66894d" class="">The data node is a block server that stores and maintains the data blocks.</p><p id="cdb2dc4a-7e9d-4ab7-9ec8-2af65303ea80" class="">Secondary name node is the component that constantly reads the meta data. It takes snapshots of data of active name node and store the data in FS Image. It is like a helper node.</p><p id="64cd688a-5a69-4aba-b14e-f938dcbda176" class="">Zookeeper: Is responsible for the maintaining of the health status of the architecture as well. Zookeeper server is a collection of all server nodes, it allows distributed processing to coordinate with each other through a shared hierarchical namespace. It also follows master slave architecture. There will ne one leader server and other slave servers.</p><p id="72f1f885-3f5f-4b6c-9aca-f836d716fcfa" class="">Erasure coding: Erasure coding (EC) is one of the methods of data protection through which the data is broken into sectors. Then they are expanded and encoded with redundant data pieces and stored across different storage media. Erasure coding adds the redundancy to the system that tolerates failures.</p><p id="27f13328-d8a3-4205-93b2-8a5e0ffeef10" class=""><strong>MapReduce Engine</strong></p><figure id="d163bd08-e7b0-446e-a77c-f46cd3bbdf74" class="image"><a href="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image3.jpeg"><img style="width:700px" src="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image3.jpeg"/></a></figure><p id="ac018a0b-e002-4459-a733-c284ccbfd01c" class="">It has Job tracker(s) as master to control the running of the Job. There are Task trackers which work as the slave nodes for the Job Tracker which control the running of the task.</p><p id="21c88b79-1ec6-4a04-b867-70afc6ae8811" class="">The job trackers take the job (operation or work), then it goes to the name node and asks for the details of files needed. Then the task is divided and given to task trackers. These smaller tasks are called map tasks; each map task is executed in each block. Therefore, the number of map tasks is equal to number of blocks.</p><figure id="3605f3ed-caa8-4e8f-993e-1a20534c2ad7" class="image"><a href="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image4.jpeg"><img style="width:700px" src="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image4.jpeg"/></a></figure><p id="0a762c41-2371-4f62-85f5-7ed1abec176a" class="">Client -&gt; job tracker -&gt; Name node -&gt; task tracker -&gt; data node -&gt; task tracker -&gt; job tracker-&gt; client</p><p id="d1294525-245a-4e1c-be81-7151481846a8" class="">Speculative execution of task:</p><p id="d25e865e-0cba-4bad-b67e-6b8b5f2e25f4" class="">Max 4 duplicate tasks (Total 5 tasks) will be run in each node then the task fails. If the duplicate tasks complete successfully in time, then the actual task is killed.</p><p id="52cf9ee3-fef1-4314-a645-c51c3b40a6f3" class="">Number of blocks = number of map tasks</p><p id="d5aa1654-6fec-461a-9e1a-3488b7a55eb6" class="">Analysis using Hadoop 2 or YARN cluster:</p><p id="b4b7f703-1145-41e5-8cfd-e10bd52ade41" class="">We use hive, pig, MRP</p><p id="a100ee49-565c-4bbe-ab30-bc1606172998" class="">Each job has one application master (AM), if AM fails the job gets failed.</p><p id="57e69a22-2f01-434b-9ca6-4cf517c9c36c" class="">Parallelly these jobs get executed.</p><p id="e132da77-511a-48a3-ac5e-fa01442c40d8" class="">The number of map reducers = number of tables. For every table one map reducer is allocated.</p><p id="37bb9c6b-6cc4-4cfe-a99a-eb25f5bb1116" class="">Resource manager is the master machine in YARN.</p><figure id="4fdf48a6-78ab-413f-b6b2-42f87dee1a37" class="image"><a href="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image5.jpeg"><img style="width:700px" src="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image5.jpeg"/></a></figure><figure id="bedcc2e8-a890-4d14-8cac-05d7812014f8" class="image"><a href="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image6.jpeg"><img style="width:700px" src="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image6.jpeg"/></a></figure><p id="e96db920-4edb-479f-96c9-0bcf60ba9173" class="">Note: -</p><ol type="1" id="6eafd5f1-cb42-4a38-a83f-0ac6188f6b94" class="numbered-list" start="1"><li>When we run the ETL pipeline in Hadoop cluster(Sqoop) there is a 4 default Map task (m 4 command option) will be executing and zero reducers will be executing in backend.</li></ol><ol type="1" id="a8dd1487-1245-46aa-93c5-0293340ad665" class="numbered-list" start="2"><li>When doing analysis in hadoop cluster number of blocks = number of map tasks executing and 1 reducer by default.</li></ol><figure id="40ae9fc7-f9f2-4af2-af4b-9e66e092c138" class="image"><a href="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image7.jpeg"><img style="width:700px" src="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image7.jpeg"/></a></figure><figure id="9a33149c-42ad-4adf-b7c1-c17fb26fcd0b" class="image"><a href="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image8.jpeg"><img style="width:700px" src="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image8.jpeg"/></a></figure><p id="b1435158-7aa5-4772-ba8f-f3f38fe964cd" class="">Process</p><ol type="1" id="104aec1e-8994-4888-8eab-399cdb4ee569" class="numbered-list" start="1"><li>Client (DE) request is sent to Resource manager (RM)</li></ol><ol type="1" id="67145675-106e-4c8d-bef0-c86f2db15ef9" class="numbered-list" start="2"><li>The RM will give the task to Active by name node(ANN), ANN will return the meta data</li></ol><ol type="1" id="f82309f0-3086-4db5-aecb-85edd1f30e3e" class="numbered-list" start="3"><li>RM will create Application Master(AM) and assign this job to it</li></ol><ol type="1" id="f6092ae8-3c2a-4a99-9556-1648a27dc18b" class="numbered-list" start="4"><li>The AM will request the resources to RM,</li></ol><ol type="1" id="bb476889-8470-4240-99b0-98f4027f34ee" class="numbered-list" start="5"><li>RM gives the containers (CPU + Memory) to AM</li></ol><ol type="1" id="bf72ec7b-19a3-4e43-ace2-b8b7fb566387" class="numbered-list" start="6"><li>AM will divide the task into smaller parts and gives the Map Tasks to the node managers</li></ol><ol type="1" id="e1aae31b-9657-4162-9605-41de81fc0c78" class="numbered-list" start="7"><li>The node managers(NM) will now give the task to nodes</li></ol><ol type="1" id="c72c9f51-68fa-4a44-9a13-264405a89543" class="numbered-list" start="8"><li>After completing the execution, NM will return the task to AM</li></ol><ol type="1" id="65fcafa7-1e30-4a53-9c67-b0dd6d42ff9f" class="numbered-list" start="9"><li>AM will now give the collected tasks to RM</li></ol><ol type="1" id="2574b789-5315-45e1-afc4-af27f9ff8ba8" class="numbered-list" start="10"><li>RM will give the result to client.</li></ol><p id="54f94815-7316-44fc-a41d-144318bb94e0" class="">Containers = Resources = Memory + Processor. These containers will be allocated by resource manager depending on the size of the file.</p><p id="3e601cfc-9a67-46da-ad51-9d1133fa9111" class="">Task = 1 block execution, small part of the entire job.s</p><p id="51137e47-7baa-4695-8168-253b3fc60822" class="">After the completion of the task the resources are given back to Resource manager by the node managers.</p><p id="1737a12f-3eff-40c1-9475-e9ad282867fc" class="">The application master is present in any one of the node managers until the completion of the job.</p><p id="70407934-e0a2-42a8-8eb1-89293f2a7c0a" class="">To schedule the jobs (ETL pipelines and analysis), we use Air Flow.</p><p id="ae25087b-cfb1-4311-b249-fa6ea0ce4140" class=""><strong>Hadoop Commands</strong></p><p id="90ff0ef4-ed44-4bd8-b7a1-c36704392bfc" class="">Hadoop commands:</p><ol type="1" id="0951e888-b00c-4745-b02a-ffd52d29fe5d" class="numbered-list" start="1"><li>To open Hadoop:<ol type="a" id="87157f99-0d99-4eb2-9c29-23d03c0dfc44" class="numbered-list" start="1"><li>Hadoop fs</li></ol></li></ol><ol type="1" id="e54b5595-e167-4731-b8d7-430e68d079bc" class="numbered-list" start="2"><li>List:<ol type="a" id="6576a51e-95b1-4ead-85e2-0ac7b50e7492" class="numbered-list" start="1"><li>Hadoop fs - ls</li></ol></li></ol><ol type="1" id="dd781f7d-3682-48a4-b6b2-fc7130f42926" class="numbered-list" start="3"><li>Remove directory:<ol type="a" id="38ce5da1-e35a-414d-8042-f07639a28b88" class="numbered-list" start="1"><li>hadoop fs -rmr &lt;diraname&gt;</li></ol></li></ol><ol type="1" id="8a84812c-5008-42e0-bcf3-f7d53dc7a02c" class="numbered-list" start="4"><li>Move LFS (Local file system) to HDFS:<ol type="a" id="27117dd7-3410-445b-8cca-289200f30ff1" class="numbered-list" start="1"><li>hadoop fs -put /home/training/rama.txt / user/training/moresalesdata</li></ol><ol type="a" id="0a1d1e89-cfcd-4867-8e7e-0491fd592c4d" class="numbered-list" start="2"><li>hadoop fs -copyFromLoacal</li></ol></li></ol><ol type="1" id="222d0a35-297a-4211-aa9e-1c57d23259a1" class="numbered-list" start="5"><li>Read data in HFS:<ol type="a" id="11828e99-2ade-4e59-ac22-d9bd781cdbb1" class="numbered-list" start="1"><li>hadoop fs -cat moresalesdata/rama.txt</li></ol></li></ol><ol type="1" id="84d44ba9-fb07-4c12-91fe-145133dd6d79" class="numbered-list" start="6"><li>Get the data into LFS from HFS:<ol type="a" id="41da2133-87e6-4767-a2a2-7958a6eaf0e1" class="numbered-list" start="1"><li>hadoop fs -get moresalesdata/titanic_data.csv /home/training/miless.csv</li></ol><ol type="a" id="21bb101f-b37c-443c-a842-42f011d03f20" class="numbered-list" start="2"><li>hadoop fs -copyToLocal<p id="09c21384-9ee3-4e00-80c0-34921df4a51b" class="">it copies the data of titanic_data.csv in hadoop into local(LFS) miless.csv file</p></li></ol></li></ol><ol type="1" id="3c92c5a1-5cbb-4cb0-a809-e75594218af7" class="numbered-list" start="7"><li>To see the node data in CLI:<ol type="a" id="c3772666-4018-4307-a534-7c14603818f0" class="numbered-list" start="1"><li>hadoop dfsadmin –report</li></ol></li></ol><ol type="1" id="9ea0072e-9c22-4dc3-ba3b-62efdb91c1d1" class="numbered-list" start="8"><li>To see the health status of blocks:<ol type="a" id="1f619689-7cdf-4de9-95b3-3e5b13ee9ae1" class="numbered-list" start="1"><li>hadoop fsck / -blocks</li></ol></li></ol><ol type="1" id="a372c9d5-db80-4120-8d81-a6f67c5d2940" class="numbered-list" start="9"><li>To check whether safe mode is on or off:<ol type="a" id="b283359d-ff4b-4645-b8e8-9a1cd6fa299b" class="numbered-list" start="1"><li>hadoop dfsadmin -safemode get</li></ol><ol type="a" id="bbb9a962-71db-439f-a74f-29b96298aa6e" class="numbered-list" start="2"><li>enter to enter into safe mode</li></ol><ol type="a" id="83e9cd2a-5c28-403e-a731-0605aeccb83d" class="numbered-list" start="3"><li>leave to leave from safemode</li></ol></li></ol><ol type="1" id="26deda99-1ad6-4b0b-9eb1-b5f4ed84b996" class="numbered-list" start="10"><li>To see the statistics of the files:<ol type="a" id="45b78fae-8ed0-45c2-9fb9-179ac4999c65" class="numbered-list" start="1"><li>hadoop fs -stat %r moresalesdata ---&gt; It shows the number of replications made in this</li></ol><ol type="a" id="d93a6bd6-b817-40c4-bde4-e3d8934dd812" class="numbered-list" start="2"><li>hdfs fsck file_name -files -locations –blocks we can see the number of blocks created, replications, locations of this file.</li></ol></li></ol><p id="17d17752-6020-4dbe-89de-5a1a95e00232" class="">Linux Commands:</p><ol type="1" id="5bc36939-25cb-4cf2-a471-6450967372a4" class="numbered-list" start="1"><li>Cat: &gt;, &lt;</li></ol><ol type="1" id="1b748f67-cffb-4ea5-b3eb-3d25efc5ac56" class="numbered-list" start="2"><li>Vi</li></ol><ol type="1" id="8faf39e1-b70e-48aa-9003-acb72aeeb83d" class="numbered-list" start="3"><li>Gedit</li></ol><ol type="1" id="2d93217f-d0a5-4ae4-a536-f1f542856695" class="numbered-list" start="4"><li>Mkdir</li></ol><ol type="1" id="17f97c77-2440-471d-b690-4091ff96ea76" class="numbered-list" start="5"><li>Move all the similarly named files: mv filename* /dir</li></ol><p id="f045dad1-d527-4c72-9594-60b0c9bdf108" class=""><strong>Map Reduce</strong></p><ol type="1" id="a53aed57-ec38-4a70-9c38-f12ff4154fb4" class="numbered-list" start="1"><li>MapReduce is a programming model that simultaneously processes and analyses huge datasets logcally into separate clusters.</li></ol><ol type="1" id="8121bc1c-ca5e-45b5-b5b8-e49274610c38" class="numbered-list" start="2"><li>If you want to analyze the data in distributed file system. We use these two functions Map and Reduce.</li></ol><ol type="1" id="ad33888d-c5a1-45bd-bb82-e4072c308206" class="numbered-list" start="3"><li>Map function splits the data by using the delimiter. The input and output of the map function is both key value pairs.</li></ol><ol type="1" id="e60204ec-c7b7-4114-9ac6-12a859bf5e9e" class="numbered-list" start="4"><li>Output of the Map function is the input of the reduce function. The reducer aggregates the data and returns. Reducer output is the final output and it is stored in HDFS.</li></ol><ol type="1" id="44c5f336-b003-413c-8e5a-76d1cb55d37b" class="numbered-list" start="5"><li>Process:<ol type="a" id="d4464678-9dd9-48dc-b2b7-6c98490bbba4" class="numbered-list" start="1"><li>Blocks -&gt; Map -&gt; shuffle -&gt; Sort -&gt; Reduce</li></ol></li></ol><ol type="1" id="fb1befb4-6a4b-4c16-9419-b570610e7cdf" class="numbered-list" start="6"><li>MapReduce Data type: long writable, int writable</li></ol><ol type="1" id="38a9f984-ac8c-423a-b9c8-6bc77ed188eb" class="numbered-list" start="7"><li>printing in MapReduce: output.Collect ()</li></ol><ol type="1" id="d4d7f69d-e828-4a0c-9e95-db53e2172dad" class="numbered-list" start="8"><li>Default number of reducers = 1</li></ol><ol type="1" id="d0b351ee-8323-4dd2-ae58-39f028dceb4c" class="numbered-list" start="9"><li><figure id="388fe760-fa04-44f4-88ef-5bc0c1dcef33" class="image"><a href="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image9.png"><img style="width:700px" src="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image9.png"/></a></figure></li></ol><ol type="1" id="92ca878c-4d17-4662-84d4-8ce235d8bf65" class="numbered-list" start="10"><li></li></ol><p id="64f9bb3b-cd42-4403-9d19-d49425ed320f" class=""><strong>Running the map reduce program</strong></p><p id="e60c0ef1-fac2-49d2-baf3-7c51e4180995" class="">Step 1: Set the Build Path of hadoop to Java</p><p id="b7e9a1ce-5e40-405b-b4ea-b46f9db26ad5" class="">Step 2: Run the configuration file</p><p id="c828c014-3731-4bec-ad56-3e33dbceb15e" class="">Step 3: Create the jar and export the jar into local</p><p id="8abe5b3c-c42b-44f2-988f-fbc07c327561" class="">Step 4: Using the program by passing an input file:</p><p id="7e6cf73c-1636-44d6-a779-00d7bffe7c89" class="">hadoop jar jarfilename.jar dirname/inputfilename output filename</p><p id="74a0b557-4917-4242-921b-94e2d6924193" class="">hadoop fs –ls outputfilename</p><p id="4a8c2bb8-dd32-46a2-bfc5-e945ef33e07d" class="">hadoop fs –cat outputfilename/part-00000</p><figure id="6737b3ab-cfc1-4fc8-868a-a95390ef0d9d" class="image"><a href="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image10.jpeg"><img style="width:700px" src="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image10.jpeg"/></a></figure><p id="8435153b-8fc4-401e-b362-83142a53027b" class="">Advantages of MapReduce</p><ol type="1" id="3aafe126-253f-40d2-9e0e-bb9183b06a77" class="numbered-list" start="1"><li>Data is processed in parallel</li></ol><ol type="1" id="0ed8812b-6199-473f-996c-131eac4e0df6" class="numbered-list" start="2"><li>Data Locality (We process the data where it is)</li></ol><p id="6bfe5cff-d682-4376-b112-e6f05840a65c" class=""><strong>MapReduce Programs</strong></p><p id="f70e1429-0fec-4d80-a2e7-6115723d44bb" class="">3 types of modes:</p><ol type="1" id="d7ba6f91-cd57-43e5-a1b5-10821ce693e1" class="numbered-list" start="1"><li>Stand alone: works in local machine, no hdfs, no configurations, no master slave architecture, just used for debugging</li></ol><ol type="1" id="e62eac78-fe8d-4331-99ab-dd8001a56720" class="numbered-list" start="2"><li>Pseudo-distributed mode: One machine will work as master and slave. It is known as single node cluster. Both name node and data node are present in the same machine. Mainly used for testing. All the daemons (Name node, data node, secondary name node, resource manager, node manager) will be running in the machine.</li></ol><ol type="1" id="cc4eab18-dfb4-485e-99d9-2e9c40a60c01" class="numbered-list" start="3"><li>Fully distributed mode: This is used in production purpose. Here the work is fully distributed across all the machines. It fully supports the master slave architecture. Here, each daemon is present in each individual machine.</li></ol><p id="b9a2f4d2-a348-491d-b8c2-e251a2d60c98" class="">Hadoop configuration files</p><ol type="1" id="22b93963-da86-4d86-a1be-7a1e6ff3b784" class="numbered-list" start="1"><li>hadoop-env.sh: Environment variables that are used in Hadoop scripts</li></ol><ol type="1" id="fc64b0d9-5b2b-4505-821e-b7ec1475b839" class="numbered-list" start="2"><li>core-site.xml: Configuration settings like I/O that are common to HDFS and MapReduce</li></ol><ol type="1" id="3885b163-42f2-46aa-93ef-59c30feecc54" class="numbered-list" start="3"><li>hdfs-site.xml: Configuration for HDFS daemons the name node and secondary name node</li></ol><ol type="1" id="5496541c-568d-4cb3-a3d1-d0f154664c98" class="numbered-list" start="4"><li>mapred-site.xml: Configuration files for MapReduce daemons, Job tracker and Task tracker</li></ol><ol type="1" id="755ccd1f-5f4e-4f2c-a2b0-ddc9b452b063" class="numbered-list" start="5"><li>master: Contains list of machines that run name node and secondary name node</li></ol><ol type="1" id="1e2e35fe-8719-4e9e-b94a-e34993902cf5" class="numbered-list" start="6"><li>slave: List of machines that run a data node and a task tracker</li></ol><p id="795593c0-d00a-4434-9f40-b5df1105a610" class="">hadoop jar /home/cloudera/Documents/hadoop-streaming-2.7.3.jar -input /user/saiprathap/word_count_py/word_count.txt -output word_count_py/output.txt -mapper /home/cloudera/Documents/mapper.py -reducer /home/cloudera/Documents/reducer.py</p><p id="95b739b7-4c58-4357-9f56-ec8c979f97f4" class=""><strong>Combiners</strong>: Mini reducers, help optimize the data transfer between the Mapper and Reducer phases by performing a preliminary reduction of data with the same key on the Mapper nodes.</p><p id="f2b406d8-0153-496f-a006-3c4f86e30108" class=""><strong>Partitioners</strong>: determine which Reducer will receive the data for each key.</p><p id="5c60a087-9828-4d72-9069-48be9b60f2ad" class=""><strong>Counters</strong>: It is a function used to gather statistics about the MapReduce Job.</p><p id="7ec26fe8-b864-4440-a3d9-6ff551461fed" class=""><strong>Structured Data analysis using MapReduce programs</strong></p><p id="a894b252-dcdf-40aa-abe9-6e7d21d2d30b" class="">Using Java:</p><ol type="1" id="6c31c904-170c-467b-81dd-08d5d8778c8f" class="numbered-list" start="1"><li>Open eclipse and write code for the map and reduce programs</li></ol><ol type="1" id="be288eb2-c5ad-4f4f-8bd0-c5c11054dfa7" class="numbered-list" start="2"><li>Run the configuration file as java application</li></ol><ol type="1" id="342a6a74-f4a6-4355-b46b-79d77db65c78" class="numbered-list" start="3"><li>Export the jar file and save it to a directory</li></ol><ol type="1" id="67f2f816-62a6-492a-8eba-b4cfdbeffe8f" class="numbered-list" start="4"><li>use this jar file and write the hadoop jar command like this:<ol type="a" id="5b86bbc5-cfd0-4a6b-8cde-a4508a36c4ee" class="numbered-list" start="1"><li>hadoop jar jarfilename.jar [local file] dirname/inputfilename [hdfs file] output filename [hdfs file]</li></ol></li></ol><p id="f9f975e4-a1e3-43da-89f6-558bffd024c9" class="">Using Python:</p><ol type="1" id="d3d3cb8d-db1d-4e10-aad2-56d7e6fa4961" class="numbered-list" start="1"><li>Write programs for map and reduce.</li></ol><ol type="1" id="896a05be-9fd0-443b-b3e5-0176336fd520" class="numbered-list" start="2"><li>Check the programs using the pipe in Linux terminal</li></ol><ol type="1" id="d57b7365-b3b9-49bc-8871-a8e638555a97" class="numbered-list" start="3"><li>put the jar file (download from geeks for geeks) and put it into the local file system</li></ol><ol type="1" id="d0c0f28f-eb44-40f6-996c-75f103469de8" class="numbered-list" start="4"><li>Now run the hadoop jar command like this:<ol type="a" id="3a75fa15-cf99-4387-867a-8b411f3f4229" class="numbered-list" start="1"><li>hadoop hadoop-streaming-2.7.3.jar[LFS] -input word_count.txt [HDFS] -output output.txt [HDFS] -mapper mapper.py [LFS] -reducer reducer.py [LFS]</li></ol></li></ol><p id="25ab95cc-fa81-4592-9f1b-6bd9f6ca2f6b" class=""><strong>Sqoop</strong></p><p id="9942211e-f4be-444b-b696-1dc1c31856f1" class="">(SQ)L + Had(oop)</p><p id="22abcf6a-c68d-40c0-bbbe-928b29f93d7b" class="">It is a tool used to transfer bulk data between HDFS &amp; relational database servers</p><p id="f965cd92-20fc-4954-a08a-82f7d31a7ac0" class="">It works on top of YARN, so it provides parallelism.</p><p id="e1592f67-943b-46a2-973e-fb80586761e3" class="">Features of Sqoop:</p><ol type="1" id="b812a1a6-6357-43c1-a677-28fa02b121bb" class="numbered-list" start="1"><li>Full Load: Loads the entire data from the database in one command</li></ol><ol type="1" id="ba4d64b8-e72d-4fcd-a80b-3fa59b6c0211" class="numbered-list" start="2"><li>Incremental Data: Loading Data from the database as it gets updated.</li></ol><ol type="1" id="dbb3e2ce-13b4-4fdc-a7ac-32102677c975" class="numbered-list" start="3"><li>It provides parallelism, it works on top of YARN</li></ol><ol type="1" id="cc1e1536-8024-4edc-81f2-d9b3782caba3" class="numbered-list" start="4"><li>It provides compression by using gzip algorithm</li></ol><ol type="1" id="c0ac47b7-3deb-460e-9f93-67d64d4e7376" class="numbered-list" start="5"><li>Kerberos Security authentication</li></ol><ol type="1" id="55561a8c-89fb-41e8-b285-2f2f793bbc69" class="numbered-list" start="6"><li>Load data directly into Apache hive for analysis</li></ol><p id="3646335a-612e-427b-a091-c35febe1bfef" class=""><strong>IMPORT</strong></p><ol type="1" id="0eecc95f-6040-458e-b539-a9a4ada5207c" class="numbered-list" start="1"><li>MySQL –u root</li></ol><ol type="1" id="11c102f9-cbf1-4a13-83bd-321d81ff978f" class="numbered-list" start="2"><li>Sqoop import –connect jdbc:mysql://localhost/employees –username root –table employees</li></ol><ol type="1" id="c11cd541-c1fa-449b-ba0e-d357aa403792" class="numbered-list" start="3"><li>Go to localhost:50070(default) and check the folders</li></ol><p id="5b2b70ce-fb9d-4e90-8831-063be55c7a13" class="">To import data into a specific directory</p><ol type="1" id="bf6e0ac6-f57c-4af3-bf41-196665c391c0" class="numbered-list" start="1"><li>Sqoop import –connect jdbc: mysql://localhost/employees –username root –table employees –m 1 – target-dir / employee10<ol type="a" id="d07aeef6-54f3-49a9-acb1-c110032782ea" class="numbered-list" start="1"><li>sqoop import --connect jdbc: mysql://10.10.205.153:3307/hr?characterEncoding=latin1 --username sai --password password --table employees --target-dir mores</li></ol></li></ol><p id="b7b34c9d-49e3-4313-890c-e3d8bd832879" class="">To import data by filtering it on the way</p><ol type="1" id="015abde7-ffe7-4da1-ab73-eb54e02bcbf1" class="numbered-list" start="1"><li>Sqoop import –connect jdbc:mysql://localhost/employees –username root –table employees –m 3 –where “emp_no &gt; 49000” – target-dir / employee10</li></ol><p id="71bd7539-cce0-462b-9486-8fe96facd9b2" class="">To import all the tables:</p><ol type="1" id="44dbeffc-e789-447f-b589-82de070e94ed" class="numbered-list" start="1"><li>Sqoop import-all-tables - -connect jdbc:mysql://localhost/employees - -username root<ol type="a" id="30090f81-2784-47b2-8d6c-f14b20212777" class="numbered-list" start="1"><li>Worked: sqoop import-all-tables --connect jdbc:mysql://10.10.205.153:3307/hr?characterEncoding=latin1 --username sai --password password --warehouse-dir mores</li></ol></li></ol><p id="d349b5e5-0328-4648-b8c8-708afae81fd6" class="">Incremental import:</p><ol type="1" id="391f74db-c617-4ea5-bd8c-03ba9d2a54cc" class="numbered-list" start="1"><li>sqoop import --connect jdbc:mysql://10.10.205.153:3307/rand?characterEncoding=latin1 --username sai --password password --table trips -m 1 --target-dir trips --incremental append --check-column id --last-value 5</li></ol><p id="cc6b0847-a375-47ad-b15d-55fbce172e49" class="">Filter and import results:</p><ol type="1" id="d98b4099-d6ce-4162-a2ad-ac455650e79a" class="numbered-list" start="1"><li>sqoop import --connect jdbc: mysql://10.10.205.153:3307/rand?characterEncoding=latin1 --username sai --password password --table trips -m 1 --target-dir trips --where &quot;id&lt;5&quot;</li></ol><p id="39344bdb-d54c-4b77-a870-e60cde255dd5" class="">Importing specific columns:</p><ol type="1" id="4a6d6808-b5b9-4c03-82dd-e6564abf3efa" class="numbered-list" start="1"><li>sqoop import --connect jdbc:mysql://localhost:3306/training --table vandana --columns &quot;id, name&quot; --target-dir vandana</li></ol><p id="bdd3c947-57e8-4bed-b31a-e246721925a6" class="">Querying:</p><ol type="1" id="f4ca3c5d-3923-4308-b1ca-a2d121ed9fc1" class="numbered-list" start="1"><li>sqoop eval --connect jdbc:mysql://locahost:3306/training --query &quot;select * from increment&quot;;</li></ol><p id="1b947922-b512-43e7-a1ef-ac9e2e6ba5df" class="">Updating:</p><ol type="1" id="6fbe9cf4-8b1d-42eb-9c3a-db7f6a9ef50c" class="numbered-list" start="1"><li>sqoop eval --connect jdbc:mysql://localhost:3306/training --query &quot;update abc set name = &#x27;Sai Prathap&#x27; where id = 123&quot;;</li></ol><p id="91180a8e-105c-4c6d-94aa-72658b36e6f0" class=""><strong>EXPORT</strong></p><p id="188e8848-8934-4656-8095-338cf9e2a30f" class="">It is mandatory that the table structure exists in SQL (target)</p><p id="48ba5427-9cd7-4e36-bd2d-abc80616a01a" class="">To see the command used to create the table use: show create table table_name;</p><p id="29de20c5-231c-4167-80bc-7c154c9020fc" class="">Command to export</p><ol type="1" id="242aa7cb-1fc2-403e-aeff-29840be23d17" class="numbered-list" start="1"><li>Sqoop export - - connect jdbc:mysql://localhost//employees - - username root - - table employees<ol type="a" id="8b1d4014-a2df-4678-9bad-3bf5ae771a7e" class="numbered-list" start="1"><li>sqoop export --connect jdbc:mysql://10.10.205.153:3307/rand?characterEncoding=latin1 --username sai --password password --table users --export-dir more</li></ol><ol type="a" id="f2f2bdca-a958-4531-9e60-b69200a0c79f" class="numbered-list" start="2"><li>sqoop export --connect jdbc:mysql://10.10.205.153:3307/rand?characterEncoding=latin1 --username sai --password password --table users --export-dir more - -input-fields-terminated-by ‘,’</li></ol></li></ol><p id="44f5e18b-f131-4182-9b7d-2ef67d8c8b00" class="">List Databases</p><ol type="1" id="999f9bf2-9ae2-410c-a6ac-a5ed5b01e1dd" class="numbered-list" start="1"><li>Sqoop list-databases –connect jdbc:mysql://localhost/employees –username root<ol type="a" id="6eba1d0c-5d29-48f8-8e26-d8597b7c450b" class="numbered-list" start="1"><li>sqoop list-databases --connect jdbc:mysql://10.10.205.153:3307?characterEncoding=latin1 --username sai --password password</li></ol></li></ol><p id="a3996d59-92f1-470e-9777-40ecb2bf2309" class="">List tables</p><ol type="1" id="39a9fb62-949d-4348-99fc-b9b57d174ea1" class="numbered-list" start="1"><li>sqoop list-tables --connect jdbc:mysql://10.10.205.153:3307/rand?characterEncoding=latin1 --username sai --password password</li></ol><p id="a82ee6dd-fd88-4ec2-96ec-3c4d866df1ae" class=""><strong>Sqoop commands</strong>:</p><ol type="1" id="90584dc2-dd39-4e0b-8cb1-9374fcc86ac2" class="numbered-list" start="1"><li>Import<ol type="a" id="37dd58c7-0946-434a-8372-187932f0fd61" class="numbered-list" start="1"><li>Source: MySQL &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; HDFS<ol type="i" id="b941f9c3-45c9-40d3-a868-61685eb1acca" class="numbered-list" start="1"><li>username: root</li></ol><ol type="i" id="5479e237-7c63-4fa0-8d01-48dcbdcda23d" class="numbered-list" start="2"><li>Password:</li></ol><ol type="i" id="fa3340d3-624f-4c42-bb40-4824adf73637" class="numbered-list" start="3"><li>MySQL server: IP address</li></ol><ol type="i" id="6eeb730d-c7a9-4664-b3d7-79db25d42cf0" class="numbered-list" start="4"><li>Driver: JDBC</li></ol><ol type="i" id="42a0b31a-e8ad-4e30-9148-65fbc27e4941" class="numbered-list" start="5"><li>Port: 3306</li></ol><ol type="i" id="f7c430c6-b8ba-4dc1-94f8-52efeb460d57" class="numbered-list" start="6"><li>Database name: training</li></ol><ol type="i" id="cfa92811-0c7f-4521-bfbc-324d491dab8a" class="numbered-list" start="7"><li>Table: countries</li></ol></li></ol><ol type="a" id="858a4aa3-0ded-4ed9-a88e-04139b508e21" class="numbered-list" start="2"><li>To import all tables, use warehouse-dir instead of target-dir</li></ol><ol type="a" id="d63d13ca-772e-4366-9945-bc6ba736095a" class="numbered-list" start="3"><li>If there is no primary key and we want to split the data into some (by default 4) parts in the HDFS system, we can use - -split-by<ol type="i" id="febd8eb0-51c9-411f-929d-2290fe6d3f73" class="numbered-list" start="1"><li>sqoop import --connect jdbc:mysql://localhost:3306/retail_db --username root --password cloudera --table order_items --split-by order_item_product_id --target-dir /user/hdfs/order_items</li></ol></li></ol><ol type="a" id="7c351656-d9d4-4bc0-8592-fdb73e0776cc" class="numbered-list" start="4"><li>There are no reducers will be executing while doing an import or export. Reducers only work during analysis only.</li></ol></li></ol><ol type="1" id="66a96e1a-80ad-4cb4-91d8-4166ea34355d" class="numbered-list" start="2"><li>Export</li></ol><ol type="1" id="c91dc063-6665-44e7-8b6f-d08984c17d9d" class="numbered-list" start="3"><li>Incremental (append &amp; lastmodified)</li></ol><ol type="1" id="af51a187-374d-4c6f-812b-0fdb0e6fd987" class="numbered-list" start="4"><li>eval</li></ol><p id="cb305f47-a478-41ed-82f5-52a8f776756f" class=""><strong>Creating Jobs</strong></p><p id="a3b0ac02-8e94-4ba8-9721-32f90c25d145" class="">Incremental Append:</p><ol type="1" id="ff818556-7b97-485b-85be-520c440be069" class="numbered-list" start="1"><li>Creating Job<ol type="a" id="138c6bd4-73f2-499c-b1bb-be4d93367805" class="numbered-list" start="1"><li>sqoop job --create incrAppend -- import --connect jdbc: mysql://localhost:3306/training --username root --table increment --target-dir increment --incremental append --check-column id -m 1</li></ol><ol type="a" id="897bb552-212b-4ef9-bb34-836a6946f590" class="numbered-list" start="2"><li>If you don’t want to create job then, you need to mention –last-value option</li></ol></li></ol><ol type="1" id="1701a3e4-a846-4cfa-9585-edf43168a05b" class="numbered-list" start="2"><li>List jobs:<ol type="a" id="b3e4fcbc-2f60-47cc-9559-65d5536d8724" class="numbered-list" start="1"><li>sqoop job - - list</li></ol></li></ol><ol type="1" id="e57ea479-cd84-4686-84fe-902ed121f80e" class="numbered-list" start="3"><li>See the details of the job:<ol type="a" id="b54275f1-d54c-4983-8880-2efad3e36221" class="numbered-list" start="1"><li>sqoop job --show incrAppend</li></ol></li></ol><ol type="1" id="9f80bda8-1234-4cb5-a066-5b982c4d3eec" class="numbered-list" start="4"><li>Execute the job:<ol type="a" id="a1ce379e-26a8-4e9a-9425-c25279465a86" class="numbered-list" start="1"><li>sqoop job --exec incrAppend</li></ol></li></ol><p id="02cc7967-bbbd-4a0d-94dd-f31a42d924f3" class="">Incremental Last Modified:</p><ol type="1" id="fe549166-603a-493a-b1f3-327b77016ece" class="numbered-list" start="1"><li>Creating job:<ol type="a" id="860d4f2a-b050-4942-8839-c23e59a20e46" class="numbered-list" start="1"><li>sqoop job --create incrementLM -- import --connect jdbc:mysql://localhost:3306/training --username root --table incrementLM --target-dir incrementLM --incremental lastmodified --check-column time -m 1 –append</li></ol><ol type="a" id="85f9e8aa-436c-403e-8445-7e7027e6d1c8" class="numbered-list" start="2"><li>If you don’t want to create job then, you need to mention –last-value option</li></ol><ol type="a" id="7b2ad413-9e38-4bbf-8d9c-a2c3f51b3a30" class="numbered-list" start="3"><li>When you create a job for incremental, the last value is stored.</li></ol></li></ol><ol type="1" id="9d880a18-8ac8-4c0e-ba65-d6da2aedcede" class="numbered-list" start="2"><li>Generating Codegen:<ol type="a" id="d728c137-e2e1-457b-bbca-2ca8c9f46790" class="numbered-list" start="1"><li>sqoop codegen --connect jdbc: mysql://localhost:3306/training --username root --table incrementLM --outdir /home/training/last_modified</li></ol></li></ol><ol type="1" id="a493d4e6-9f28-4db0-9ab5-8eaed70a3412" class="numbered-list" start="3"><li>Merging the files to get update:<ol type="a" id="75303f4d-4fb4-48d6-886e-cc9ad749bbad" class="numbered-list" start="1"><li>sqoop merge --new-data /user/training/incrementLM/part-m-00001 --onto /user/training/incrementLM/part-m-00000 --target-dir incrementLMmerged --jar-file /tmp/sqoop-training/compile/4b5fec17d7ff321b3ee7afd3f94c1fbc/incrementLM.jar --class-name incrementLM --merge-key id</li></ol><ol type="a" id="bcfc962a-bbae-49f5-aaad-abdedfd1c187" class="numbered-list" start="2"><li>You can see this jar file location while compilation of the sqoop codegen</li></ol><ol type="a" id="fb982f78-a5e2-4d6e-b626-d02085710b76" class="numbered-list" start="3"><li>–new-data, --onto, --jar-file, --class-name, --merge-key</li></ol></li></ol><p id="45a8b979-426d-442b-ad43-dcfc8790a6a7" class=""><strong>FLUME</strong></p><figure id="edf29437-5128-4d0d-a2a8-c6aef1b96b22" class="image"><a href="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image11.jpeg"><img style="width:700px" src="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image11.jpeg"/></a></figure><p id="1873b4e4-abf9-4a65-80d3-c5835dadcb48" class="">It is used to bring live data from web servers. It brings data in unstructured data. As it is loading data from a dynamic source, flume will not stop automatically like sqoop.</p><p id="1ecd55e8-13de-407f-816c-9a12bf61e0b9" class="">The channel is buffer, it is RAM. The data is stored temporarily before going to HDFS.</p><p id="95a7bc7a-bccb-42d9-87ec-01eb4ccfca42" class="">Agent file is a configuration file, it has: source + channel + Sink</p><p id="094fda86-aef7-4816-a64f-05cc7d3fc706" class="">We can run the agent file using the command FLUME –ng</p><p id="d2a06b29-5e5f-4455-a1c2-7c93e793eba2" class="">Flume puts the data in HDFS but Kafka puts data into NoSQL.</p><p id="45cf3507-5276-455a-8dc4-c7872cd584ac" class=""><strong>Slowly changing dimensions</strong>:</p><p id="6ccc2f07-16f3-45ff-a4eb-787458a2f06c" class="">SCD1: Historical data is not maintained</p><p id="92e534fa-0a69-4d47-82e7-3e48075b7a95" class="">SCD2: Historical data is maintained by storing new(updated) record.</p><p id="66e78aeb-7375-47e1-af84-db9154ebdd01" class="">SCD3: Loading the data into as new record by changing some columns and keeping a flag.</p><p id="cdbcc1f8-2533-4ec3-821b-47e6429d3da1" class="">These strategies will not work for ETL pipeline while bringing the live data. Once data is stored in Big data database we can apply these strategies.</p><figure id="f3f90a0f-b686-409d-989a-f1096198261f" class="image"><a href="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image12.png"><img style="width:700px" src="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image12.png"/></a></figure><p id="50c34d63-ad7c-4c66-972d-11d48bc42c3e" class=""><strong>HIVE</strong></p><p id="b3493f91-eed1-4a27-8cfc-2a3b8926ab25" class="">Documentation: <a href="https://hive.apache.org/">https://hive.apache.org/</a></p><p id="6c699169-7275-4d0d-8f68-51b0d6b3c556" class="">Course: <a href="https://www.simplilearn.com/learn-hadoop-spark-basics-skillup?utm_campaign=Hadoop-rr17cbPGWGA&amp;utm_medium=Description&amp;utm_source=youtube">https://www.simplilearn.com/learn-hadoop-spark-basics-skillup?utm_campaign=Hadoop-rr17cbPGWGA&amp;utm_medium=Description&amp;utm_source=youtube</a></p><p id="76b302fd-8c5a-4390-ab56-24fb8fd3ce7d" class="">Hive is a data warehouse on Hadoop. It is the first Big data warehouse. It is open source.</p><p id="1da024f0-6c1b-441b-841f-d60afb1a74b0" class=""><strong>Database and Data Warehouse</strong>:</p><ol type="1" id="8c5a8f64-6b87-42b1-b368-d5d9728de534" class="numbered-list" start="1"><li>A data warehouse is used for decision making purpose. It stores historical data.</li></ol><ol type="1" id="7fc4d2c5-8fe3-4903-aea2-5455237d16ce" class="numbered-list" start="2"><li>Traditional data warehouses are not built on top of DFS. It can handle only TBs of data. They are based on single server.</li></ol><ol type="1" id="f17ac03a-66d6-48d5-883d-6965488d64ef" class="numbered-list" start="3"><li>Data warehouse is OLAP used for analysis purpose. Databases are OLTP, they are used for application purpose.</li></ol><ol type="1" id="e4e2e5fb-d064-46aa-b75d-b3c7e8f3e340" class="numbered-list" start="4"><li>Data warehouse in azure synaps, in aws red shift.</li></ol><p id="0a8e1aea-3658-4cdc-af2c-67f211ab34fb" class="">Hive is a dependent API, it depends on MapReduce programs and HDFS to store data.</p><p id="fafec8dc-0e98-4243-9ec5-39be6b829a82" class="">Mainly HIVE is used to process structured data and sometimes semi-structured can also be processed.</p><figure id="bf1ead1e-f4ec-40e8-b24a-b16622e12d55" class="image"><a href="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image13.jpeg"><img style="width:513px" src="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image13.jpeg"/></a></figure><ol type="1" id="1165b345-fc76-44a7-9bf8-18b1fd28cd2b" class="numbered-list" start="1"><li><strong>Thrift Server:</strong> The Hive Thrift Server (HiveServer2) is a service that exposes Hive functionality through Thrift, a framework for building cross-language services. It allows clients to connect remotely and submit queries to Hive, making it possible to interact with Hive from various programming languages.</li></ol><ol type="1" id="10748701-6d23-49d5-82dd-86e65a910662" class="numbered-list" start="2"><li><strong>Metastore:</strong> The Hive Metastore is a centralized repository that stores metadata about Hive tables, partitions, and schemas. It contains information about table structures, column data types, storage locations, and more. The Metastore is essential for query optimization and schema management. It stores the schema of the tables stored in the HDFS. If you make any changes in the HIVE table, the changes also made effective in Meta store.</li></ol><figure id="ce927dde-e7d8-4242-a8dd-1c2e3ce2b1f0" class="image"><a href="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image14.png"><img style="width:700px" src="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image14.png"/></a></figure><p id="9f82e908-edc6-4408-845c-e3a7778b3cd8" class="">When you write a HIVE query, the backend is converted into MapReduce in Hadoop cluster.</p><p id="61351190-f1ae-4194-84d1-90c28244431e" class=""><strong>Hive Data types</strong>:</p><p id="285a309d-0b43-41ec-b9a3-04c359365ac8" class="">There are two types of data types: 1) Primitive 2) Complex datatypes</p><ol type="1" id="bb90e13b-74a7-4072-8885-a5d6bdf6d300" class="numbered-list" start="1"><li>Primitive: int, float, double, long, string</li></ol><ol type="1" id="131a60a6-d996-4be4-b120-f42fa4bf89d5" class="numbered-list" start="2"><li>Complex: Array, List, Structs</li></ol><p id="717f3974-0a17-4ad3-b3db-65a4e6e3be11" class="">All sql commands work in hive except: insert, update, delete as per hive 2.2 because, it works on top of HDFS, so there is no modification of the data can be done on the data.</p><p id="d4cfbd5b-d930-4b02-8528-4a3736755bf7" class="">Hive 2.3 onwards it supports partial ACID properties.</p><p id="fb3c730c-4d89-4072-9da5-6c3708653d4a" class="">We can create a table in hive but cannot insert data into it. We can just load the data into the table from HDFS. When you load the data into the warehouse, the data in the HDFS is deleted. If you want to see the data use hive only (Hive folder). When you load the data from LFS the data is not deleted.</p><p id="b8165fcd-4b08-4ece-bfb0-5e12ac9ca688" class="">So, if I want to get the data from any relational database, we can directly import the data into HIVE instead of HDFS but, in the backend the data is stored in the HDFS only.</p><p id="4ab0cd5f-b304-430c-9a10-8832fa4eb3ed" class="">Note: There are two joins map side join and reduce side join for internal optimization implemented by MapReduce engine. When one table is small and the other table is large, then the map side join gets executed. When both the tables are big then reducer side join is executed.</p><p id="d4f8d9e4-8fa4-4bdd-93c9-f16824b02836" class=""><strong>Working with HIVE from HDFS</strong>:</p><ol type="1" id="f6ef942d-e2c1-41b1-82ad-048c576627e0" class="numbered-list" start="1"><li>Put the data into hive</li></ol><ol type="1" id="48a95cdc-00d6-47e9-bab9-4182639ea82d" class="numbered-list" start="2"><li>open hive console and create table using a command like this:<ol type="a" id="bba43b33-2538-48d0-97d4-9d95e042c10f" class="numbered-list" start="1"><li>create table house_survey (id int, city string, country string) row format delimited fields terminated by &#x27;,&#x27;;</li></ol></li></ol><ol type="1" id="b7f0b001-3da0-4897-8a91-86fa367fd521" class="numbered-list" start="3"><li>Load data into the table by using this command<ol type="a" id="ab6dd093-d702-4fb9-9375-81cf30413359" class="numbered-list" start="1"><li>load data inpath &#x27;/user/saiprathap/House_Survey.csv&#x27; into table house_survey;</li></ol></li></ol><ol type="1" id="98f851fe-92a4-4d0f-be60-b10bc47912d9" class="numbered-list" start="4"><li>select * from house_survey;</li></ol><p id="dc921ee4-658d-44b2-a82f-cc21ea5970a4" class=""><strong>Import data from MySQL:</strong></p><ol type="1" id="94eac138-e676-4186-9dad-1ddd10b7f81b" class="numbered-list" start="1"><li>Single table import<ol type="a" id="9b387d62-fb51-43d8-a6dc-fb6f1ff6986b" class="numbered-list" start="1"><li>sqoop import --connect jdbc: mysql://localhost:3306/retail_db --username root --password cloudera --table customers --hive-import -m 1</li></ol></li></ol><ol type="1" id="ef1b8fd3-b660-4ce8-9883-dfbca2005120" class="numbered-list" start="2"><li>Import all tables<ol type="a" id="5f0f05a0-f99b-4c39-86fc-edc859c7ce73" class="numbered-list" start="1"><li>Firstly, create a database in hive and use - -hive-database option</li></ol><ol type="a" id="a98250aa-7f19-4fcf-8277-acc7bb342a6e" class="numbered-list" start="2"><li>sqoop import-all-tables --connect jdbc: mysql://localhost:3306/retail_db --username root --password cloudera --hive-import --hive-database retail_db -m 1</li></ol></li></ol><p id="231d1d83-687d-4f7b-a556-a7ae89bf59a0" class="">Storing the output of the queries into a table:</p><ol type="1" id="ac49b3dc-5838-4e40-bfd4-cc69526ac0aa" class="numbered-list" start="1"><li>Create an empty table</li></ol><ol type="1" id="0e8fa5fc-1dc3-4b7e-b2bd-6e1e119c2c62" class="numbered-list" start="2"><li>Run this command:<ol type="a" id="3dece6de-2a2e-4a2e-80ac-d2d1bd2d77f2" class="numbered-list" start="1"><li>insert overwrite table op1 select name, sum (bill1 + bill2 + bill3 + bill4) from host group by name;</li></ol><ol type="a" id="09d04734-c5f3-4cdf-8943-cc58a2fe624c" class="numbered-list" start="2"><li>when you use a cte:<ol type="i" id="2dcd4a54-cd08-48bb-998c-09be3f867085" class="numbered-list" start="1"><li>with cte as (select name, sum(bill3) as b from host group by name) insert overwrite table op3 select name, b from cte order by b desc limit 1;</li></ol></li></ol></li></ol><p id="31c48252-d304-493d-b337-f29212b38d5d" class=""><strong>Export data to MySQL:</strong></p><ol type="1" id="3e7d8474-33fc-4a61-a70e-ab251d1b25d2" class="numbered-list" start="1"><li>create table in MySQL</li></ol><ol type="1" id="e4df0c62-d485-4cfe-ab52-970d1e570ee4" class="numbered-list" start="2"><li>Run this command<ol type="a" id="0118ba78-c6ed-4098-b3e4-455e2f89a007" class="numbered-list" start="1"><li>sqoop export --connect jdbc: mysql://localhost:3306/hive --username root --password cloudera --table op1 --export-dir /user/hive/warehouse/op1/000000_0 --input-fields-terminated-by &#x27;\0001&#x27;</li></ol></li></ol><p id="87f38390-ef1e-4e7b-ad73-1870e144f65e" class="">There are two types of tables in hive warehouse</p><ol type="1" id="22ec217f-3113-42f3-976f-c092eacede3a" class="numbered-list" start="1"><li>Internal tables (Managed tables)</li></ol><ol type="1" id="acd13d8e-719c-4357-ba9e-c5877bc8fbdd" class="numbered-list" start="2"><li>External tables (Non managed tables)</li></ol><p id="614960f8-fbea-42ab-8b82-ce1c3d7f7755" class=""><strong>Internal tables</strong>:</p><ol type="1" id="d384fe73-2306-4304-bf2e-671bea2c0984" class="numbered-list" start="1"><li>They are also known as managed tables, because we have total control of the table.</li></ol><ol type="1" id="3ffb8886-a1a9-4478-ae1f-7562a5f70424" class="numbered-list" start="2"><li>The tables which are stored in warehouse are called as internal tables (Normal tables). We can manage everything like inserting, etc.</li></ol><ol type="1" id="bda9a8fb-d069-4173-8c89-28f9e6a3e5e3" class="numbered-list" start="3"><li>If you drop an internal table then file, table and metadata will be deleted.</li></ol><ol type="1" id="2273d86e-7c0b-4414-83bb-64d105762827" class="numbered-list" start="4"><li>It is recommended to use internal tables for analysis.</li></ol><p id="282e6c63-6068-4245-8ab0-97c04ee14c3e" class=""><strong>External table</strong>:</p><ol type="1" id="49c58dd2-2b6e-4b7e-a477-c58ff9fb35d0" class="numbered-list" start="1"><li>The table which are stored externally. This table can be created using external keyword.</li></ol><ol type="1" id="2d04022b-0486-410b-9feb-e1adc9c49dec" class="numbered-list" start="2"><li>When you drop a table, only the metadata will be deleted not the actual table.</li></ol><ol type="1" id="a6d587ed-a0cf-4bd3-b43c-d3667969611a" class="numbered-list" start="3"><li>The external tables are used to store the output of analysis done in Hive.</li></ol><ol type="1" id="f7991c67-2504-4b13-be11-bed543a2ca95" class="numbered-list" start="4"><li>Command to create external table:<ol type="a" id="625d3637-19bd-4401-a4e4-c87d066e9c2d" class="numbered-list" start="1"><li>create external table external_table (id int, city string) row format delimited fields terminated by &#x27; &#x27; location &#x27;/user/hive/warehouse/external_table/sample.txt&#x27;;</li></ol><ol type="a" id="6c452b82-8fc2-45bb-99ad-5e480a4a333e" class="numbered-list" start="2"><li>Now drop the table and check for the tables. The table will not be present but the file of the data will be present in the HDFS file system. So, use external tables only to store the output don’t use it for input of analysis. Because, we cannot use it as a table, operations like incremental import etc. cannot be done on the external table.</li></ol></li></ol><ol type="1" id="16bead4a-5d87-4b5f-804f-d730a5fb4c5f" class="numbered-list" start="5"><li>Command to create external table in the location where the data is present.<ol type="a" id="4e99e193-dc65-40ee-b2f9-ce5359cdfe25" class="numbered-list" start="1"><li>By using this type of creation, we need not load the data manually in the table.</li></ol><ol type="a" id="3c82eb4a-8023-4f63-b252-6314272ae6f8" class="numbered-list" start="2"><li>To create external table and put the data automatically we need to first put the data into HDFS (non hive folder) and create a table with specifying the directory of the file, not up to filename. Please note that, there should be only one file inside that folder specifying one table. If there are multiple files, then we cannot insert data automatically. We will get an error saying that, not a directory.</li></ol><ol type="a" id="cde4376a-1d68-4874-84bc-478136246fd6" class="numbered-list" start="3"><li>hadoop fs -put sample.txt /user/cloudera/salesdata</li></ol><ol type="a" id="3bdc6b64-a69c-46a4-91a7-dc9a935e65e0" class="numbered-list" start="4"><li>create external table myextb (id int, city string) row format delimited fields terminated by &#x27; &#x27; location &#x27;/user/cloudera/salesdata&#x27;;</li></ol><ol type="a" id="f094ab68-95f4-4c00-b1fd-715dcc73cfce" class="numbered-list" start="5"><li>Give the location up to directory level only not up to the file level.</li></ol><ol type="a" id="7934c7b6-44e7-4a50-877d-7f2cad4db697" class="numbered-list" start="6"><li>select * from myextb;</li></ol></li></ol><ol type="1" id="f7a11bb8-c128-422d-84d1-e4ee6ba52aef" class="numbered-list" start="6"><li>To create external table and store the output into it, the process is same as the insert overwrite of a normal table:<ol type="a" id="b76c2bcd-2d7d-4d0d-a473-2eb6c4c671aa" class="numbered-list" start="1"><li>Firstly, create an external table:</li></ol><ol type="a" id="6cfbf97f-2c66-4b07-9ec6-e24aaf3b3d8a" class="numbered-list" start="2"><li>create external table result1(pname string, totalbill int) row format delimited fields terminated by &#x27; &#x27; location &#x27;/user/hive/warehouse/result1&#x27;;</li></ol><ol type="a" id="f2f378c8-874b-4916-9c08-c65de912d54f" class="numbered-list" start="3"><li>Then overwrite the result into that table:<ol type="i" id="b83ba92f-189e-4394-875e-24b31366b89b" class="numbered-list" start="1"><li>Insert overwrite table result1 select name, sum (bill1 + bill2 + bill3 + bill4) from host group by name;</li></ol></li></ol></li></ol><p id="ae0c51d2-9f7b-4865-bb8b-85c64a8d72c9" class="">Hive modes: Local mode (single node) and MapReduce mode (more than one node)</p><p id="6a0ef372-9e63-410a-8c36-ab95ca97356e" class=""><strong>Hive Optimization Techniques</strong></p><ol type="1" id="1c10e405-0ac1-4033-a1cf-cd57769b05ed" class="numbered-list" start="1"><li>Hive partitions</li></ol><ol type="1" id="dbe196c1-4858-408d-a720-373e86f7fc9e" class="numbered-list" start="2"><li>Bucketing</li></ol><ol type="1" id="9bffc906-3768-4f4b-80f5-a4b581610291" class="numbered-list" start="3"><li>File formats</li></ol><ol type="1" id="e22c650b-f1d7-4fc5-ac2a-1ecd7c8440e6" class="numbered-list" start="4"><li>Indexes</li></ol><p id="cf8e9a33-7d04-47c1-af40-e9cb18988eb1" class=""><strong>Hive Partitions</strong>:</p><ol type="1" id="5e41eb04-4838-4e3e-b821-7be22ae09fcd" class="numbered-list" start="1"><li>It is one of the optimization techniques in hive to improve the performance of queries when the table is large and when you have to write multiple queries on that table.</li></ol><ol type="1" id="d0e4cdc9-a134-4d00-bb8e-1fadfefae08c" class="numbered-list" start="2"><li>We create partition table for the table that is already present in the hive. We can create partition table on both internal and external table but it is best to create partition on internal table.</li></ol><ol type="1" id="1623eda5-5efe-44ea-89e7-9d33f6bdce0c" class="numbered-list" start="3"><li>There are two types of partitions dynamic and static partitions.</li></ol><ol type="1" id="c96d5b38-488c-406c-92e1-309132f9e180" class="numbered-list" start="4"><li>You need to decide which column has the high important to create a partition on. You can create partition based on more than one column. You should use the column which is used to partition in where, group by, etc. clauses in every query.</li></ol><ol type="1" id="0e567fb3-83f1-4571-8d1a-c1d2fd2b5e4b" class="numbered-list" start="5"><li>The partitioned table is stored as a directory in HDFS. Number of directories will be dependent on the number of distinct values of the partitioned column.</li></ol><ol type="1" id="38cbbaae-71b8-4f81-8a25-6f21608bf9b6" class="numbered-list" start="6"><li>Note: Partitioning or any optimization technique is not allowed by default in hive, we need to activate it by using set commands.</li></ol><ol type="1" id="08c4c60f-456e-4d00-ab20-613831d68bf2" class="numbered-list" start="7"><li>In non-partitioned tables, hive would have to read all the files in a table’s data directory and then apply filters. This is more time taking and expensive.</li></ol><ol type="1" id="3749c674-6876-46b8-953b-5881b704b266" class="numbered-list" start="8"><li>When a partitioned table is queried then only the relevant directories are read. If you filter data based on the column that is not used to partition, then it will be more expensive than a non-partition query. So, don’t over partition the data, it will cause slow performance and it will be more burden for the name node.</li></ol><ol type="1" id="1a2eb203-bb4f-49f2-b092-c516712e36e3" class="numbered-list" start="9"><li>Partitioning multiple columns create hierarchical directories.</li></ol><ol type="1" id="b01a74b6-b7f6-4cf5-b005-e0bf34939af8" class="numbered-list" start="10"><li>Setting the partition<ol type="a" id="e66e571b-7fb0-40a6-93fe-ffd720d21e4a" class="numbered-list" start="1"><li>SET hive.exec.dynamic.partition = true;</li></ol><ol type="a" id="a729c1a4-9315-461d-92ec-491ba0608535" class="numbered-list" start="2"><li>SET hive.exec.dynamic.partition.mode = nonstrict;</li></ol><ol type="a" id="77bc7581-bf02-4698-b9ae-5e6059bdbd9a" class="numbered-list" start="3"><li>SET hive.exec.max.dynamic.partitions = 5000;</li></ol><ol type="a" id="7e2aeafd-1c8a-4295-952a-94bc817ad9ba" class="numbered-list" start="4"><li>SET hive.exec.max.dynamic.partitions.pernode=5000;</li></ol></li></ol><p id="8b2fb862-b874-402c-88a3-03580e73386b" class="">Types of partitions:</p><ol type="1" id="bb6ae75c-668f-4c3d-91bf-91e8e4cff53a" class="numbered-list" start="1"><li>Static partitions:<ol type="a" id="21f4076d-31ae-45d5-9ae7-9a0fb1863f61" class="numbered-list" start="1"><li>In static partitions we need to insert the data manually for each partitions.</li></ol><ol type="a" id="6e150732-0d2b-4fe6-9cf8-dce5df746889" class="numbered-list" start="2"><li>Say you are partitioning with city column; the you need to do this for each city (partition)</li></ol><ol type="a" id="71551515-7b8c-45a7-8004-513bbe009bcc" class="numbered-list" start="3"><li>insert into table table_name partition (city = ‘Chennai’) select name, age from original_table where city = ‘Chennai’;</li></ol></li></ol><ol type="1" id="aff2664a-c48e-4dce-80de-cb7aba1fbbf2" class="numbered-list" start="2"><li>Dynamic partitioning:<ol type="a" id="937dd607-8b65-436f-9b88-b635dae216cf" class="numbered-list" start="1"><li>It is automatic, we don’t need to insert the data manually into each partition.</li></ol><ol type="a" id="627dfdce-a6aa-4a76-9479-ad645b279927" class="numbered-list" start="2"><li>All we do in this notes are dynamic partition only.</li></ol></li></ol><p id="921d150c-748b-41be-a4c5-db9294dd0c42" class=""><strong>Partitioning process (Dynamic)</strong>:</p><ol type="1" id="044888c8-0466-4411-92cc-3a409194d6c9" class="numbered-list" start="1"><li>Creating table:<ol type="a" id="7a29cb17-389b-4dc6-816e-57bc5f9f0de4" class="numbered-list" start="1"><li>create table part (doj string, room int, bill1 int, bill2 int, bill3 int, bill4 int) partitioned by (name string) row format delimited fields terminated by &#x27; &#x27;;</li></ol></li></ol><ol type="1" id="eaad3793-8a83-448f-af02-51f22ee37b98" class="numbered-list" start="2"><li>creating partitions:<ol type="a" id="4c4055de-eada-45bd-a99d-412396d72416" class="numbered-list" start="1"><li>insert overwrite table part partition(name) select doj, room, bill1, bill2, bill3, bill4, name from host;</li></ol></li></ol><ol type="1" id="522ff17a-bcc0-49af-9818-c2d0ec823c8e" class="numbered-list" start="3"><li>It is good to maintain the last column as the partition column to identify the column.</li></ol><ol type="1" id="befafe0e-4c5a-4acd-af70-383a30afde09" class="numbered-list" start="4"><li>To see the partitions on a table, the use: show partitions table_name;</li></ol><ol type="1" id="bd9aa643-540a-464d-b17f-3bbdd278e4ac" class="numbered-list" start="5"><li>Now open the file system and see the number of directories and files created.</li></ol><p id="79be56e1-79b0-4fff-9dd1-9c1e4923b91e" class=""><strong>Hive Bucketing</strong>:</p><ol type="1" id="c62892b1-5ed6-4b64-ab99-2790cd83fc4c" class="numbered-list" start="1"><li>In this we can mention the number of parts of the data we want to make.</li></ol><ol type="1" id="6c85c5f4-4afd-4c29-ad5d-8e7f03bcf429" class="numbered-list" start="2"><li>The data is stored in file formats, not in the directories format.</li></ol><ol type="1" id="769d1058-075d-4597-b2a5-6d3d3483a978" class="numbered-list" start="3"><li>The data is divided into each bucket based on the hashing algorithm, the result will be going to a particular bucket based on the hash value of the value of the column.</li></ol><ol type="1" id="0973618c-4693-4416-8fe9-8110b0a5941d" class="numbered-list" start="4"><li>Bucketing is also not enabled by default, we need to run this command<ol type="a" id="55c7d357-f1ae-491d-aa12-fe21e797a2e2" class="numbered-list" start="1"><li>set hive.enforce.bucketing = true</li></ol></li></ol><ol type="1" id="38964238-e274-4bac-afee-4f62ef62c1c6" class="numbered-list" start="5"><li>Creating hive buckets<ol type="a" id="87cb8300-957b-441b-b119-6f0d25dfd8d0" class="numbered-list" start="1"><li>create table y (id int, name string, country string) clustered by(country) into 9 buckets row format delimited fields terminated by ‘,’;</li></ol><ol type="a" id="d5e9370b-6a7f-4c44-a9d7-f5a72e781954" class="numbered-list" start="2"><li>insert overwrite table y select id, name, country from x;</li></ol></li></ol><ol type="1" id="83fccbb9-ed68-498c-96bb-41748874c06f" class="numbered-list" start="6"><li>Worked<ol type="a" id="72e9f607-030d-4ec1-8f10-7c033c9205fd" class="numbered-list" start="1"><li>create table bucketBbsalesdataRegion (Region string, Country string, Item_Type string, Sales_Channel string, Order_Priority string, Order_Date string, Order_ID string, Ship_Date string, Units_Sold int, Unit_Price float, Unit_Cost float, Total_Revenue float, Total_Cost float, Total_Profit float) clustered by(Region) into 3 buckets row format delimited fields terminated by &#x27; &#x27;;</li></ol><ol type="a" id="aa89fcf2-3ce5-49da-a2ff-eeb170b08e1f" class="numbered-list" start="2"><li>insert overwrite table bucketBbsalesdataRegion select region, country, Item_Type, Sales_Channel, Order_Priority, Order_Date, Order_ID, Ship_Date, Units_Sold, Unit_Price, Unit_Cost, Total_Revenue, Total_Cost, Total_Profit from bbsalesdata;</li></ol></li></ol><ol type="1" id="00fa7917-0fc8-49dd-95b6-f70b64be91a7" class="numbered-list" start="7"><li>Sampling queries:<ol type="a" id="81a35e24-8a1f-4d0c-8b7a-efc34d02db36" class="numbered-list" start="1"><li>When you want to query the results from only specific buckets then we use sampling queries.</li></ol><ol type="a" id="34edadeb-b2d8-4e48-960e-f93c55dfb334" class="numbered-list" start="2"><li>But the catch is you need to know which bucket you want to choose.</li></ol><ol type="a" id="f1a5e5c3-bfe4-4d3f-a40e-1d24b57ae17c" class="numbered-list" start="3"><li>select region, id from table_name TABLESAMPLE (bucket 3 out of 5 on region)</li></ol></li></ol><ol type="1" id="9c9354a2-5c45-4a3d-a054-612342770032" class="numbered-list" start="8"><li>Decide the number of buckets = CEIL ( log2(table_size/128) )<ol type="a" id="968421e7-8e7e-4e59-a5e7-47744dd2fe2b" class="numbered-list" start="1"><li>table_size is in MB</li></ol></li></ol><p id="ffb97fc5-1651-4aca-a952-ca9c38971a3d" class=""><strong>Partition and Bucketing at one go</strong>:</p><p id="4813fbda-8715-4cfe-9b69-56ee3828a3a4" class="">create table table_name (id int, amount int, country string) partitioned by (name string) clustered by(country) into 3 buckets row format delimited fields terminated by &#x27;,&#x27;</p><p id="00f7be15-f8a4-4789-8d6d-2f672f6525b8" class=""><strong>Hive Indexing</strong></p><p id="c89f3154-ceaf-483f-a237-b8cd0ea25909" class="">The goal of hive indexing is to improve the speed of query lookup on certain columns of a table. Indexes are pointers on a particular column</p><p id="ce5423c2-737a-4c9a-aec4-a745e1da87be" class="">Two types of indexing in HIVE:</p><ol type="1" id="e31ecd18-e80b-4455-8846-abb17b409927" class="numbered-list" start="1"><li>Compact index (most commonly used): Very less buffer ram in hive warehouse no matter the size of the data</li></ol><ol type="1" id="85160ff6-6ff3-494a-947a-2be2a3be9af4" class="numbered-list" start="2"><li>Bit map index: Takes very high buffer ram.</li></ol><p id="568166fe-cc86-408c-a1b6-a3c9173d6b03" class="">It is best practice to delete the index after analysis, because it takes the buffer memory. We cannot do indexing in partitioned and bucketed tables</p><p id="4231fd59-781a-4101-8c9c-fb2e149e765d" class="">Worked:</p><ol type="1" id="92266686-0c77-42ef-8fc5-ec3a60d6ac51" class="numbered-list" start="1"><li>create index index_origin on table airlines(origin) as &#x27;org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler&#x27; with deferred rebuild;</li></ol><ol type="1" id="2f66138f-9a83-4721-a8d5-2ef5e7ed7051" class="numbered-list" start="2"><li>show formatted index on airlines;</li></ol><ol type="1" id="638137c8-9305-4d06-8457-0538dcb179a4" class="numbered-list" start="3"><li>drop index index_origin on airlines;</li></ol><p id="77951276-4ee5-4f40-9148-2867921f3b85" class=""><strong>File Formats</strong></p><figure id="0e1e44c0-6311-493c-a691-c0c896d94a1f" class="image"><a href="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image15.png"><img style="width:700px" src="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image15.png"/></a></figure><ol type="1" id="0db0d1f0-ae4c-4676-a8ec-78cb28bb6908" class="numbered-list" start="1"><li>Parquet is compressed with Snappy, ORC file is compressed with Gzip. Before RC file Avro file was there, it produces only 8% compression.</li></ol><ol type="1" id="cb0d677b-35fc-45ba-98dd-dba85c52b37a" class="numbered-list" start="2"><li>Compressing:<ol type="a" id="ac421644-2e08-4603-997b-f8b1f7d71b54" class="numbered-list" start="1"><li>Text:<ol type="i" id="2db97e77-d42b-4a28-b094-3a4a1b4f3447" class="numbered-list" start="1"><li>beeline</li></ol><ol type="i" id="1d187a5d-4991-481f-9521-31f8d61add90" class="numbered-list" start="2"><li>! connect jdbc:hive2://localhost:10000</li></ol><ol type="i" id="de3a58f5-0112-405a-87dd-51c253895433" class="numbered-list" start="3"><li>create external table xyz (id int, name string, amount int) row format delimited fields terminated by &#x27;,&#x27; stored as textfile location &#x27;/user/saiprathap/cat&#x27; tblproperties (&quot;skip.header.line.count&quot;=&quot;1&quot;);</li></ol><ol type="i" id="5eecfd9c-ca66-48a8-9665-55afcacca0a1" class="numbered-list" start="4"><li>You can also create an internal table; it is not mandatory that the table is external. Stored as text file is not mandatory to put, y default it is stores in text file</li></ol><ol type="i" id="251dc5e8-44a2-4209-9074-2a7a5ee6cf72" class="numbered-list" start="5"><li>show tblproperties xyz;</li></ol></li></ol><ol type="a" id="db865383-d427-4e3e-a481-0ea54a730a7e" class="numbered-list" start="2"><li>Avro File:<ol type="i" id="92459aa7-b393-434f-9aee-2dc9596d0897" class="numbered-list" start="1"><li>beeline</li></ol><ol type="i" id="e7ddbaf1-bb3c-4aa0-9eff-03dc04a6fdec" class="numbered-list" start="2"><li>create table my_avro (id int, name string, age int) stored as avro;</li></ol><ol type="i" id="c6eb0ad1-46e0-4f07-9f68-522a7a1d432f" class="numbered-list" start="3"><li>insert overwrite table my_avro select * from xyz;</li></ol><ol type="i" id="ee40a4ea-f041-4b10-9539-3251287fd385" class="numbered-list" start="4"><li>select * from my_avro;</li></ol></li></ol><ol type="a" id="58919be6-50ea-4e21-ad99-958586c224d1" class="numbered-list" start="3"><li>Parquet file:<ol type="i" id="98cda156-3600-48c9-a6a1-56fb0f8ef8fd" class="numbered-list" start="1"><li>beeline</li></ol><ol type="i" id="6dcc618b-1282-4e64-bb58-97000713990f" class="numbered-list" start="2"><li>create table my_parquet (id int, name string, amount int) stored as parquet;</li></ol></li></ol><ol type="a" id="a9c4ef71-e6ad-4f24-bf46-f65493b6cc20" class="numbered-list" start="4"><li>ORC file:<ol type="i" id="93a97306-a6da-422f-a6c3-6686acfba4cd" class="numbered-list" start="1"><li>beeline</li></ol><ol type="i" id="6c7c8f2f-f8bf-4f1f-81db-c910871a7e07" class="numbered-list" start="2"><li>create table my_orc (id int, name string, amount int) stored as orc;</li></ol></li></ol><ol type="a" id="5bf89447-fbc6-46b0-9421-dfeaeafea03e" class="numbered-list" start="5"><li>Now check the storage of these tables, parquet takes very less storage than others.</li></ol><ol type="a" id="93653fda-5a95-42a0-bf94-24466d2a50d6" class="numbered-list" start="6"><li>Do the above process for a large table?</li></ol></li></ol><p id="857e87b1-822b-448a-8142-b55791cb4315" class=""><strong>Semi Structured data analysis</strong>:</p><ol type="1" id="a35d2053-d389-4114-a05d-88b9068cfd89" class="numbered-list" start="1"><li>We use serde jar to use semi structured data analysis with hive. SERDE: serialization and De serialization, it is used to convert semi structured to structured data (in hive, not in spark)</li></ol><ol type="1" id="69b89866-a938-45ec-8ad8-d95bb33b1a05" class="numbered-list" start="2"><li>Download the jar file (it is there in Big data folder) into the LFS</li></ol><ol type="1" id="96ef5932-57a1-4a49-889b-71be58ca39a9" class="numbered-list" start="3"><li>now add the jar file to hive using the command<ol type="a" id="b933f1d9-c1eb-4c4c-a7c0-bc60c6e65d65" class="numbered-list" start="1"><li>ADD JAR json-serde-1.3.7.jar;</li></ol></li></ol><ol type="1" id="2358939f-1bec-48f3-b351-d204b8bb2042" class="numbered-list" start="4"><li>create table json_nested (country string, languages array&lt;string&gt;, religions map&lt;string, array&lt;int&gt;&gt;) row format SERDE &#x27;org.openx.data.jsonserde.JsonSerDe&#x27; stored as textfile;</li></ol><ol type="1" id="47e0fa81-3203-405e-8328-7b1f3ac3ddb5" class="numbered-list" start="5"><li>load data inpath &#x27;/user/saiprathap/nestedtext.txt&#x27; into table json_nested;</li></ol><p id="5180f6d5-2ff2-4c98-9f4d-46403e29a1e3" class=""><strong>Hive Joins</strong></p><p id="0759d60e-629f-47e7-b3e1-5eeb47b302e7" class="">Two types of joins for join queries they are: i) map side join ii) reduce side join</p><p id="3be0860a-333c-42fe-8f4d-3b90daf327d9" class="">When one table is small and the other is large then internally map reduce engine executes map side join to join these tables. When the two tables are large then the reduce side join is used internally.</p><p id="e0a34bfa-b4cf-468e-b009-271a29919d30" class="">In map side join the small table is taken in the cache of the memory (distributed cache)</p><p id="70e98ea1-2b21-41fc-a1cd-9033475b0176" class="">Worked:</p><p id="8e494ddb-e09d-491d-896f-52b4abf13a96" class="">Two data sets used: NYSE_daily.txt, NYSE_dividends.txt</p><ol type="1" id="ed58754e-6237-4360-adb3-7fee28b56c50" class="numbered-list" start="1"><li>create table NYSEdailyhead (stexchange String, stock_symbol String, stock_date String, stock_price_open double, stock_price_high double, stock_price_low double, stock_price_close double, stock_volume int, stock_price_adj_close double) row format delimited fields terminated by &quot;\t&quot; lines terminated by &quot;\n&quot; tblproperties(&quot;skip.header.line.count&quot;=&quot;0&quot;);</li></ol><ol type="1" id="92821412-6dc3-436b-8b50-858adc17ec23" class="numbered-list" start="2"><li>create table nyse_dividends (divstock_exchange STRING, divstock_symbol STRING, divstock_date STRING, dividends FLOAT) row format delimited fields terminated by &#x27;\t&#x27;;</li></ol><ol type="1" id="924271af-4a27-4d89-9261-df62e7e4d6b9" class="numbered-list" start="3"><li>Load data into the tables</li></ol><ol type="1" id="5a395a3c-7a07-4ee6-8090-bcdc0a07be05" class="numbered-list" start="4"><li>Map side join:<ol type="a" id="1ce7b477-4110-4cf2-8bfc-caf62aadecdf" class="numbered-list" start="1"><li>select /*+MAPJOIN(nyse_dividends) */ * from nysedailyheads a join nyse_dividends b on a. stock_symbol = b. divstock_symbol and a. stock_date = b. divstock_date where a. adj_close &lt;= 20;</li></ol><ol type="a" id="c6a619eb-3b32-4ca8-8dfc-cb8d9341726a" class="numbered-list" start="2"><li>The above command has +, so the comment is excluded</li></ol></li></ol><p id="b3bb95a0-8375-4515-95da-cd1bc702d6c1" class=""><strong>Hive commands</strong></p><ol type="1" id="586ffcd1-a715-424d-8068-e5ce29557f0a" class="numbered-list" start="1"><li>create table table_name (col datatype, …) row format delimited fields terminated by ‘,’ tblproperties (“skip.header.line.count” = ”1”);</li></ol><ol type="1" id="3fd5b1a4-10b3-4297-b744-e10f88a7a1eb" class="numbered-list" start="2"><li>show tables</li></ol><ol type="1" id="4a0a7223-46e5-483b-9838-7c8440a7b017" class="numbered-list" start="3"><li>describe table_name</li></ol><ol type="1" id="3cccf96c-740c-47f0-8d55-ff7effce99ce" class="numbered-list" start="4"><li>load data inpath &#x27;/user/saiprathap/House_Survey.csv&#x27; into table house_survey;</li></ol><ol type="1" id="4d348f7c-60c4-453e-ad4f-1a9f4116db80" class="numbered-list" start="5"><li>insert overwrite table op1 select * from house_survey;</li></ol><ol type="1" id="cc4f80f7-10de-47ff-92b0-e190ae851a4b" class="numbered-list" start="6"><li>External table:<ol type="a" id="a0b5bd6a-c40f-4741-8571-689a59f078f6" class="numbered-list" start="1"><li>create external table external_table (id int, city string) row format delimited fields terminated by &#x27; &#x27; location &#x27;/user/hive/warehouse/external_table/sample.txt&#x27;;</li></ol></li></ol><ol type="1" id="a13aa030-e5dd-4aed-a7f2-7bb8d5e91c0f" class="numbered-list" start="7"><li>Partition:<ol type="a" id="3ad63c70-9030-425b-bd07-1df2c5b06ec9" class="numbered-list" start="1"><li>create table part (doj string, room int, bill1 int, bill2 int, bill3 int, bill4 int) partitioned by (name string) row format delimited fields terminated by &#x27; &#x27;;</li></ol><ol type="a" id="2dfd16bd-0ebe-47ae-a8e1-fd8508657a35" class="numbered-list" start="2"><li>set hive.exec.dynamic.partition=true;</li></ol><ol type="a" id="ce5d67ab-31a6-4cda-aca2-c44e2bf7fc36" class="numbered-list" start="3"><li>set hive.exec.dynamic.partition.mode=nonstrict;</li></ol><ol type="a" id="0e3d1d66-d1f5-42e3-bc1e-ddb673feb30b" class="numbered-list" start="4"><li>insert overwrite table part partition(name) select doj, room, bill1, bill2, bill3, bill4, name from host;</li></ol></li></ol><ol type="1" id="7c14050d-c9d2-40d7-af7a-5f0d3e811b88" class="numbered-list" start="8"><li>Bucketing:<ol type="a" id="0db6c023-3e65-4d72-bde0-9337e6991f6c" class="numbered-list" start="1"><li>create table y (id int, name string, country string) clustered by(country) into 9 buckets row format delimited fields terminated by ‘,’;</li></ol></li></ol><ol type="1" id="5f469246-6a98-4b92-b3d1-e8ba5ba311a1" class="numbered-list" start="9"><li>File formats:<ol type="a" id="df97b66c-ee55-4607-9926-836fb6df781a" class="numbered-list" start="1"><li>create table xyz (id int, ...) stored as parquet/ avro/ textfile</li></ol></li></ol><p id="2a52e021-e278-4319-883c-b7eb1d60f2b2" class=""><strong>HBASE</strong></p><ol type="1" id="e353acaa-e8ab-49c3-a089-1f03e58d1b9a" class="numbered-list" start="1"><li>Database: OLTP, Transactional, SQL</li></ol><ol type="1" id="473576ff-a7b5-4d55-a24c-61dd45d8ae35" class="numbered-list" start="2"><li>Data warehouse: OLAP, Historical, Hive</li></ol><ol type="1" id="2c4bb04e-94f7-4acf-a7bc-d04f5b628001" class="numbered-list" start="3"><li>Big Data Database: OLAP,Transactional and analytical, NoSQL</li></ol><ol type="1" id="9df98739-b726-40ea-acad-430661e0d584" class="numbered-list" start="4"><li>Big data warehouse: Data lake</li></ol><ol type="1" id="e91ac79a-fdfc-4b06-a8bd-434f18d4a0e8" class="numbered-list" start="5"><li>Data Lake: Data is a centralized repository that allows to store vast amounts of structured and unstructured data at any scale.</li></ol><ol type="1" id="b70be9b0-902f-45fa-a05a-a5d79f455027" class="numbered-list" start="6"><li>Delta Lake: Data lake + ACID properties</li></ol><ol type="1" id="c38cba54-4c80-41fc-9c09-bade7e9cc2a3" class="numbered-list" start="7"><li>Delta Lakehouse: Data lake + data warehouse</li></ol><ol type="1" id="e66d1b64-c18c-49a1-adf8-c9c72abaf064" class="numbered-list" start="8"><li>Direct lake = Delta lake + BI tools</li></ol><ol type="1" id="b2d369e4-8820-4bf1-a9e1-adb57897e277" class="numbered-list" start="9"><li>NoSQL Technologies: HBase, MongoDB, Cassandra, Couch DB, Neo4J. In AWS cloud: Dynamo DB, Azure: Cosmos DB.</li></ol><ol type="1" id="a52bfc75-94a9-414b-889d-f8fcb45a5b4a" class="numbered-list" start="10"><li>HBase is the first NoSQL database in this IT world.</li></ol><ol type="1" id="9569cc4e-e213-485a-bb42-81d01e196b48" class="numbered-list" start="11"><li>A data warehouse cannot be used as a backend for application. We can use this NOSQL as a backend for an application and analysis as well.</li></ol><ol type="1" id="414e3b03-80fe-4d95-b818-525e699337f5" class="numbered-list" start="12"><li>HBase is OLAP, it is column oriented DB. It can store any large volume of structured and semi structured data.</li></ol><ol type="1" id="fe497a25-02be-4e10-9ad6-d1e5eacdcaf9" class="numbered-list" start="13"><li>NoSQL is schema less. It stores the data as key value storage.</li></ol><ol type="1" id="b0e4c058-4e93-46d2-9d6b-03602a27c900" class="numbered-list" start="14"><li>HBase is an independent API, it does not depend on MapReduce.</li></ol><ol type="1" id="726b41eb-4d40-4803-9320-a01458198346" class="numbered-list" start="15"><li>HBase = HDFS + HBase API</li></ol><ol type="1" id="232f6190-0f61-4526-88f2-8ab5ea5bd55d" class="numbered-list" start="16"><li>HBase API architecture is master-slave.</li></ol><ol type="1" id="7e63dfd1-4c2e-43f0-b451-4a064bcf5940" class="numbered-list" start="17"><li>It has HBase query language.</li></ol><ol type="1" id="a6cb5ccd-04ab-4fee-8dd3-0a73cbfe8a21" class="numbered-list" start="18"><li>It has support to connect with BI tools.</li></ol><p id="a7799fcc-abf8-4f8d-9dac-2bd7b8186e84" class=""><strong>HBase Architecture</strong></p><figure id="36749d24-aaa4-4e6f-9b9d-2bf76f6c2696" class="image"><a href="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image16.jpeg"><img style="width:543px" src="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image16.jpeg"/></a></figure><p id="6cabafc9-d134-41d5-b7f6-ef8b059fa1e0" class="">HBase master server: It is like name node. It stores the metadata of every table.</p><p id="6ce82221-d314-48bb-8b51-b836bb1cee5f" class="">The master server sends the request to the region server and the region server executes the task.</p><figure id="10aa2931-09f5-4480-8713-69d03b3951ec" class="image"><a href="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image17.png"><img style="width:700px" src="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image17.png"/></a></figure><p id="bd840a03-8894-44fb-8eb9-9a9288363a54" class="">Region servers maintain column families as regions. There can be any number of column families.</p><p id="1ce4bb85-e23e-40cb-856d-abb8c55d6059" class="">In HBase, tables are split into regions and are served by the region servers. Regions are vertically divided by column families into “Stores”. Stores are saved as files in HDFS.</p><p id="4dbee4c8-8892-4453-87bd-0bc9b73ef43c" class="">Tables, Rows, Column families, columns, Cells: value, versions, timestamp</p><p id="928a37b6-9541-43bc-ab2a-78050aaf8f48" class="">HBase commands: Create (create table with column families), Delete (delete specific rows), list, put (insert update), get (select), scan (retrieve complete table data), enable, disable, describe, drop.</p><p id="f2759963-4810-413d-beab-54e9e7fbe802" class="">The region server manages all the column families created. When you create a table the table by default it is enabled, if you want to delete the table then you need to first disable it.</p><p id="467782f5-ac92-4790-8db6-be56f4893ead" class="">We cannot have random read and write in HDFS, in HBase we can have random read and write.</p><p id="493cd1c8-066b-4925-8dca-78a213af74c6" class=""><strong>Importing from MySQL</strong></p><ol type="1" id="2baf7d50-57d8-4b1b-afe7-8fead3f4dbe1" class="numbered-list" start="1"><li>For Creating table with one column family<ol type="a" id="0e6cf50c-7ff0-4a37-8113-efa6dbf1a7bb" class="numbered-list" start="1"><li>sqoop import --connect jdbc: mysql://localhost:3306/retail_db --username root --password cloudera --table categories --hbase-table &#x27;categories&#x27; --column-family category_details --hbase-create-table --columns category_id,category_department_id,category_name --hbase-row-key category_id -m 1</li></ol></li></ol><ol type="1" id="3238329d-c26f-45e0-9cef-32be6c684cc2" class="numbered-list" start="2"><li>For creating multiple column families<ol type="a" id="0922071a-4f1a-4ccd-a797-96fda5a86e32" class="numbered-list" start="1"><li>sqoop import \</li></ol><ol type="a" id="20106fee-43cc-4fee-af6b-e80e961f5020" class="numbered-list" start="2"><li>-connect jdbc:mysql://localhost:3306/retail_db \</li></ol><ol type="a" id="a36c82e2-69c5-4a19-87ca-e7ec024e755b" class="numbered-list" start="3"><li>-username root \</li></ol><ol type="a" id="da58a4b8-fe22-4238-9c21-a12761c1babe" class="numbered-list" start="4"><li>-password cloudera \</li></ol><ol type="a" id="85a619e1-7bbf-4fe1-a075-03a6679a6c0e" class="numbered-list" start="5"><li>-table categories \</li></ol><ol type="a" id="d608c152-23e5-4780-a81a-387bfc326c24" class="numbered-list" start="6"><li>-hbase-table &#x27;categories&#x27; \</li></ol><ol type="a" id="615b528c-1c6b-4081-ba7b-a53da70de1a6" class="numbered-list" start="7"><li>-hbase-create-table \</li></ol><ol type="a" id="e6758808-df26-4ed8-986b-3277e5498e18" class="numbered-list" start="8"><li>-columns category_id,category_department_id,category_name \</li></ol><ol type="a" id="b32b39c2-2042-481c-af54-6ed587bcacac" class="numbered-list" start="9"><li>-hbase-row-key category_id \</li></ol><ol type="a" id="ccd7c116-fb40-45b1-8dbc-30acdaf10cf5" class="numbered-list" start="10"><li>-hbase-column-mapping category_id:category_details,category_department_id:additional_details \</li></ol><ol type="a" id="55d3eb2a-a85a-4c1f-96fc-e3672f22c6a9" class="numbered-list" start="11"><li>m 1</li></ol></li></ol><p id="de9dea17-8525-4739-9feb-492c60df1941" class="">Importing data from HDFS flat file:</p><ol type="1" id="b1ce6eb4-426f-4bcf-a706-a4b2997c3bde" class="numbered-list" start="1"><li>Create the table in HBase and mention the required column families<ol type="a" id="6de0f9af-4e47-44ea-9012-894d45a8cf58" class="numbered-list" start="1"><li>create ‘sample’ ‘personal’</li></ol></li></ol><ol type="1" id="c8951b7f-ee00-46aa-98f8-266d0f046183" class="numbered-list" start="2"><li>Now write this command in normal terminal not in HBase<ol type="a" id="aa95c26d-a0a9-484c-a3dd-6bfa8def5cba" class="numbered-list" start="1"><li>hbase org.apache.hadoop.hbase.mapreduce.ImportTsv</li></ol><ol type="a" id="0aa35490-61c5-4419-b98d-0f5af8fa7f6e" class="numbered-list" start="2"><li>Dimporttsv.separator=&#x27;,&#x27; -Dimporttsv.columns=&quot;HBASE_ROW_KEY,personal:lname,personal:age&quot;</li></ol><ol type="a" id="b0197086-20a4-4da3-872f-07a504554c96" class="numbered-list" start="3"><li>sample</li></ol><ol type="a" id="ad74dbe0-9ad8-4224-b40d-b5d73f8b6306" class="numbered-list" start="4"><li>&#x27;/user/saiprathap/sample.csv&#x27;</li></ol></li></ol><ol type="1" id="0d57d69c-bcea-4281-9d06-ec381740842a" class="numbered-list" start="3"><li>Check the data in HBase. The first column in the file is automatically taken as the row key.</li></ol><ol type="1" id="66cd67ae-d139-4396-97ff-a1ce6a3392e1" class="numbered-list" start="4"><li>If you want to change the row key column, then place the HBASE_ROW_KEY in the required position of the column. (If you want to metion the 2nd column as row key then first mention the first column and then mention HBASE_ROW_KEY in second place).<ol type="a" id="777ec531-1ca9-4ed9-8b12-ffa26f162c3e" class="numbered-list" start="1"><li>Dimporttsv.columns= &quot;personal:fname, HBASE_ROW_KEY, personal:age&quot;</li></ol></li></ol><p id="d214ad14-9ae3-4f78-8786-fcbace730e28" class=""><strong>Hive table to HBase</strong></p><ol type="1" id="5f5fe98c-48d1-40f2-a177-f800b260a595" class="numbered-list" start="1"><li>HBase is not suitable to perform complex queries, so we integrate HBase with Hive</li></ol><ol type="1" id="57fb5f06-dbe5-4682-b7bf-82819d2d620a" class="numbered-list" start="2"><li>Hive HBase integration with HBase storage handler with serde properties</li></ol><ol type="1" id="ce646666-1456-4599-af13-6a7c7cd66302" class="numbered-list" start="3"><li>Worked:<ol type="a" id="9a586692-b044-4001-8419-5db4c080e0ed" class="numbered-list" start="1"><li>Create an HBase recognized table in Hive with the column families mentioned in the HBase table<ol type="i" id="f38ff423-73f0-425b-aaf8-4617640e2c14" class="numbered-list" start="1"><li>create table hbase_table(key string, city string, cost int) row format delimited fields terminated by &#x27;,&#x27; stored by &#x27;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#x27; with serdeproperties (&quot;hbase.columns.mapping&quot;=&quot;:key,location:city, price:cost&quot;) tblproperties(&quot;hbase.table.name&quot;=&quot;hbase_demo&quot;);</li></ol></li></ol><ol type="a" id="4bcb7ecb-1a89-4afe-b034-ef4627e627e1" class="numbered-list" start="2"><li>Now load the data into new hive table using a normal table, it will be automatically reflected inside the HBase table<ol type="i" id="629a6510-0355-470c-a73b-437e22a691e7" class="numbered-list" start="1"><li>insert into hbase_table select * from hbase_demo;</li></ol></li></ol><ol type="a" id="a5e17e0c-aff3-4ba1-9f85-22b0209c323b" class="numbered-list" start="3"><li>Check the data in HBase, a new table is created in HBase, the data will be there automatically in the HBase</li></ol><ol type="a" id="0a6959b8-c507-4b0c-81b0-49181ed07c01" class="numbered-list" start="4"><li>Don’t mention the key column in mapping, the left out column is automatically taken as row key.<ol type="i" id="872ca44d-12b7-4354-81e3-e78e28ed9453" class="numbered-list" start="1"><li>create table hbase_table(key string, city string, cost int) row format delimited fields terminated by &#x27;,&#x27; stored by &#x27;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#x27; with serdeproperties (&quot;hbase.columns.mapping&quot;=&quot;location:city, price:cost&quot;) tblproperties(&quot;hbase.table.name&quot;=&quot;hbase_demo&quot;);</li></ol></li></ol></li></ol><ol type="1" id="0782d3e7-6450-4715-9cb3-6859ffcd9e10" class="numbered-list" start="4"><li>The Hive table and HBase table are both synchronized i.e., what ever the changes made in the hive are reflected in HBase table. If you drop table in Hive then the table in HBase is also deleted.</li></ol><p id="119e41d2-a070-4115-8353-4630eb0a7cc3" class="">HBase table to Hive</p><ol type="1" id="d85ea313-1583-426a-8cfd-d630117bb0b7" class="numbered-list" start="1"><li>Create an external table in Hive<ol type="a" id="aabb0377-d2cf-4d31-843c-632b1ae6c4bf" class="numbered-list" start="1"><li>create external table hbase_cars(key int, name string, price int, rating int, year int, color string, brand string) stored by &#x27;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#x27; with serdeproperties(&quot;hbase.columns.mapping&quot;=&quot;details:name, details:price, details:rating, details:year, details:color, details:brand&quot;) tblproperties(&quot;hbase.table.name&quot; = &quot;cars&quot;);</li></ol></li></ol><ol type="1" id="7116de0e-6d3d-413a-97fc-f51aadd1d583" class="numbered-list" start="2"><li>This automatically brings the data into hive table from HBase</li></ol><p id="2fe3d33e-dd34-4951-8153-cdd713e480de" class=""><strong>HBase commands</strong></p><ol type="1" id="b37989ed-b5de-47fb-88b4-6631df7090c3" class="numbered-list" start="1"><li>list: to list out tables.</li></ol><ol type="1" id="673a250a-f7f0-437a-9519-2f2b31694497" class="numbered-list" start="2"><li>scan table_name: to print the entire table.</li></ol><ol type="1" id="95b1736b-2e37-4529-a2b8-3a4c5590f1c6" class="numbered-list" start="3"><li>create ‘table_name’, ‘column_family_name’: To create a table with Column families, you can mention any number of column families separated with comma.</li></ol><ol type="1" id="47ce3624-80e3-41f6-b9e8-030c1e37aea8" class="numbered-list" start="4"><li>describe ‘table_name’: to get the details of the table.</li></ol><ol type="1" id="687c94de-a78b-47b3-8ca1-3303c6ad7aee" class="numbered-list" start="5"><li>disable ‘table_name’</li></ol><ol type="1" id="62f4cf33-ad3b-4a54-b4dd-dd1d7f5d806c" class="numbered-list" start="6"><li>drop ‘table_name’</li></ol><ol type="1" id="bb66a547-47d2-4992-8dc0-4aa82f42fe63" class="numbered-list" start="7"><li>Inserting:<ol type="a" id="7b9a44d2-6a2c-497d-a742-24e1ee63a43c" class="numbered-list" start="1"><li>put &#x27;cars&#x27;, &#x27;123&#x27;, &#x27;details:name&#x27;, &#x27;Brezza&#x27;</li></ol></li></ol><ol type="1" id="a6975313-a080-4b0f-9ca8-40af52e74d07" class="numbered-list" start="8"><li>Updating:<ol type="a" id="a8fb980f-72f8-4a93-9110-5d49dd488f8c" class="numbered-list" start="1"><li>put &#x27;cars&#x27;, &#x27;123&#x27;, &#x27;details:name&#x27;, ‘Swift’</li></ol></li></ol><ol type="1" id="9753c415-2d93-452e-8c47-27924ad0227c" class="numbered-list" start="9"><li>Delete the data from the table:<ol type="a" id="121361f2-3d89-497e-80e0-fcf3fe57dd2f" class="numbered-list" start="1"><li>delete &#x27;cars&#x27;, &#x27;123&#x27;, &#x27;details: price&#x27;</li></ol></li></ol><ol type="1" id="3155ce3d-ec6d-4327-a343-b10086f78b5e" class="numbered-list" start="10"><li>To get the data from a specific row and a specific column<ol type="a" id="3d346b7d-2dfb-446b-be38-bf18e1668bc4" class="numbered-list" start="1"><li>get &#x27;host&#x27;, &#x27;3&#x27;, {COLUMN=&gt; &#x27;patient_details: name&#x27;}</li></ol></li></ol><ol type="1" id="34ba9fc0-0161-44f9-9ee7-b75d0bbb21e5" class="numbered-list" start="11"><li>Getting some columns:<ol type="a" id="dabc0743-e73e-4d64-99cb-8dae3cf9b69e" class="numbered-list" start="1"><li>get &#x27;host&#x27;, &#x27;1&#x27;, {COLUMN =&gt; [&#x27;bill_details: rBill&#x27;, &#x27;bill_details: mBill&#x27;]}</li></ol></li></ol><ol type="1" id="b1ca1a42-486f-4407-88ba-b24e559bd13d" class="numbered-list" start="12"><li>Getting some columns within a time<ol type="a" id="ade4c83d-3da2-4bbd-81f0-8e43d5d9e19b" class="numbered-list" start="1"><li>range get &#x27;host&#x27;, &#x27;1&#x27;, {COLUMN =&gt; [&#x27;bill_details: rBill&#x27;, &#x27;bill_details: mBill&#x27;], TIMERANGE =&gt; [1694430107619,1694430156228]}</li></ol></li></ol><ol type="1" id="c63f09c1-c948-43b0-8c7f-29437f743914" class="numbered-list" start="13"><li>Limit:<ol type="a" id="af5853dc-b097-4e98-a3d4-2c983b703886" class="numbered-list" start="1"><li>scan &#x27;categories&#x27;, {&#x27;LIMIT&#x27; =&gt; 5}</li></ol></li></ol><ol type="1" id="a436e0fb-b58e-4920-826c-f78377397b31" class="numbered-list" start="14"><li>Setting the number of versions to be maintained:<ol type="a" id="ef8ba905-4e51-4f9a-a623-bd89bf50382a" class="numbered-list" start="1"><li>alter &#x27;cars&#x27;, {NAME =&gt; &#x27;details&#x27;, VERSIONS =&gt; 100}</li></ol></li></ol><ol type="1" id="f05b95f6-1954-4fff-ab7d-74efc61793e6" class="numbered-list" start="15"><li>Getting the number of versions:<ol type="a" id="3cf031e8-25bc-422a-b295-3b76305f6be1" class="numbered-list" start="1"><li>scan &#x27;cars&#x27;, {VERSIONS =&gt; 3}</li></ol></li></ol><ol type="1" id="8a9593d4-7c54-46c5-ac65-5c9b43b5fc43" class="numbered-list" start="16"><li>Getting the versions in get:<ol type="a" id="7bf8269b-6951-4986-a325-961005ce91a8" class="numbered-list" start="1"><li>get &#x27;cars&#x27;, &#x27;123&#x27;, {COLUMN =&gt; &#x27;details:price&#x27;, VERSIONS =&gt; 3}</li></ol></li></ol><ol type="1" id="189bbb65-daec-4da8-a89b-6851dbfdf5a5" class="numbered-list" start="17"><li>Selecting some columns and time range:<ol type="a" id="78e2d613-22a9-4f06-92e9-023f4cab9043" class="numbered-list" start="1"><li>scan &#x27;cars&#x27;, {COLUMN=&gt;[&#x27;details:name&#x27;,&#x27;details:color&#x27;] ,TIMERANGE =&gt; [1694428574271, 1694428666332]}</li></ol></li></ol><ol type="1" id="9304ef7a-92ca-4b20-b5a4-f730098a46bf" class="numbered-list" start="18"><li>Adding a column family:<ol type="a" id="6a6ca603-f268-467a-b67b-078e235f70f0" class="numbered-list" start="1"><li>First disable the table</li></ol><ol type="a" id="7f9f82b8-e7b3-4f3f-9f46-a3bc6781caa8" class="numbered-list" start="2"><li>alter &#x27;cars&#x27;, {NAME =&gt; &#x27;xy&#x27;}</li></ol></li></ol><p id="cf62ec2a-cf49-47f0-a13d-bafc23eff25e" class=""><strong>Snowflake</strong></p><ol type="1" id="defb24d8-0ddc-4ff2-84c6-366b15d81226" class="numbered-list" start="1"><li>There are two data models are there, star schema and snowflake schema. Star schema allows limited amount of normalization. But snowflake allows high normalization.</li></ol><ol type="1" id="189d4ffb-0b93-4c0b-92e4-d06418c09720" class="numbered-list" start="2"><li>Snowflake is a data warehouse. It is also a data model (Snowfalke schema).</li></ol><ol type="1" id="e2c35b65-71e3-4c9c-974b-825be24e30e0" class="numbered-list" start="3"><li>RDMS like MySQL use star schema, but snowflake uses snowflake schema.</li></ol><ol type="1" id="61aa07bb-caab-4c1f-85bf-f24b2dcbca86" class="numbered-list" start="4"><li>Snowflake is good in performance, when having multiple table and large volume of data.</li></ol><ol type="1" id="65deea5d-584f-48d9-860a-6961c6e301b6" class="numbered-list" start="5"><li>Snowflake uses very less storage.</li></ol><ol type="1" id="39a1b28a-638c-49e5-9506-68de07aa4a37" class="numbered-list" start="6"><li>Every table by default stores as columnar storage. By default data is compressesd into some format like ORC.</li></ol><ol type="1" id="f7c0bc1a-70ba-4545-95aa-343a8dd22ab8" class="numbered-list" start="7"><li>You can use snowflake to analyse structured, semi-structured and unstructured data.</li></ol><ol type="1" id="ebe31aac-52f0-422b-a3f5-4937e764f76a" class="numbered-list" start="8"><li>It is completely a cloud based data warehouse.</li></ol><figure id="4029eb53-e788-4652-b1db-44e71d3eac67" class="image"><a href="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image18.png"><img style="width:700px" src="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image18.png"/></a></figure><p id="9ccce42a-fd98-43b4-8149-44c81e169bae" class=""><strong>Snowflake Architecture</strong></p><ol type="1" id="4bfc6a75-0dc3-4fb0-a44e-a0c57662a707" class="numbered-list" start="1"><li>Fact Table: Most of the times fact table is used to store numerical data.</li></ol><ol type="1" id="7b64c6d8-2c6e-4334-9b29-b6bca6cc5f00" class="numbered-list" start="2"><li>Dimension table: Every dimension table is related to fact table.</li></ol><ol type="1" id="1db21256-53d6-4937-8485-a8672b5af674" class="numbered-list" start="3"><li>We can do analysis in snowflake using both SQL and Python.</li></ol><ol type="1" id="19312580-f785-481e-aa4a-94606f5a6596" class="numbered-list" start="4"><li>*One cluster can have only one warehouse.</li></ol><ol type="1" id="4e241641-8a82-41c4-88ee-008a1909d41b" class="numbered-list" start="5"><li>It processes the data in disk, if you want to use memory processing we can use snowflake connector and connect to spark to perform processing in memory.</li></ol><ol type="1" id="8deb1dc0-b36f-48a0-bd81-e97ca59948dd" class="numbered-list" start="6"><li>There are 3 layers in the snowflake:<ol type="a" id="64956476-3be9-4e8d-9944-5de8ddbd55fc" class="numbered-list" start="1"><li>Storage Layer (AWS – S3, Azure – Datalake, GCP – GCS)</li></ol><ol type="a" id="72ac6a5a-90fd-4c40-8138-21c45f975cf5" class="numbered-list" start="2"><li>Virtual warehouse: Compressed column</li></ol><ol type="a" id="e23b3d6a-cde6-430b-b84d-9881fddb653c" class="numbered-list" start="3"><li>Access control layer</li></ol></li></ol><ol type="1" id="af25632a-307a-4cee-a8a7-b923e2b8267d" class="numbered-list" start="7"><li>Snowflake architecutre is also called as multi cluster shared disk architecture.</li></ol><p id="412ffcbc-06f1-42c9-b184-79c6b365e8f4" class="">Differences between Star schema and Snowflake schema</p><table id="d25d775d-4197-4195-bfcc-94a773b4b9cb" class="simple-table"><tbody><tr id="8bb63a68-b66d-4b29-89e9-8eb3e9438692"><td id="QA=D" class=""><strong>Star Schema</strong></td><td id="fTsk" class=""><strong>Snowflake schema</strong></td></tr><tr id="b5c1cd74-2878-43dd-beb2-6ef3538c9cc5"><td id="QA=D" class="">Partially normalization design, because only one dimension schema.</td><td id="fTsk" class="">Highly normalized schema, because multiple sub dimension schema.</td></tr><tr id="8f2768ee-7f94-476f-af77-bd48e0b57664"><td id="QA=D" class="">It requires more storage</td><td id="fTsk" class="">It uses less storage space, because of default compression</td></tr><tr id="8725eb50-0abb-4eee-8270-dcd968a7d0a9"><td id="QA=D" class="">Scalability is limited</td><td id="fTsk" class="">High scalability</td></tr><tr id="18c1908d-9149-4cde-8e17-e87f10c1d856"><td id="QA=D" class="">It is always good for simle queries</td><td id="fTsk" class="">It is good for complex queries</td></tr><tr id="ef411dc8-1b75-4d3d-b82f-b9473a3d8348"><td id="QA=D" class="">Limited volume of data</td><td id="fTsk" class="">High volume of data</td></tr></tbody></table><p id="c8dbf064-063f-4762-9b53-26298e0db888" class=""><strong>Micro Partitions:</strong></p><ol type="1" id="4487bbac-17a3-4f18-bf51-5f45a71a0bb1" class="numbered-list" start="1"><li>When you upload a 500MB data into snowflake it takes 50MB. Even this 50MB data is taken into micro partitions.</li></ol><ol type="1" id="94fa6d81-0efe-49bc-a87e-2d5474449d66" class="numbered-list" start="2"><li>In micro partitions, the data will be divided into blocks (column-wise) in physical structure of the snowflake data warehouse.</li></ol><ol type="1" id="06e874f3-b69d-4573-8168-e36003b28e46" class="numbered-list" start="3"><li>If a table has 24 rows and 4 columns, then it is can be divided into 4 micro partitoins. Each micro partition can contain 4 rows, each column will be divided into separate parts and placed in different places.</li></ol><ol type="1" id="e43a45cb-2e21-4aed-873b-324f628c5e0a" class="numbered-list" start="4"><li><figure id="813e98e4-747d-4293-b2b6-7e82cb548ffd" class="image"><a href="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image19.png"><img style="width:700px" src="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image19.png"/></a></figure></li></ol><p id="6e59dfbb-0884-471b-bad8-8765f4209674" class=""><strong>Bringing data into Snowflake:</strong></p><p id="1a1490c8-1c9b-447d-9dbd-b7ac6e4a82b1" class="">Two types of stages for data transfer:</p><ol type="1" id="02ec23d6-acf7-475a-b441-470f8060e5fb" class="numbered-list" start="1"><li>Internal stage: Loading the local file to snowflake<ol type="a" id="7375b6a3-9923-4900-b43e-1e4df30ab7ef" class="numbered-list" start="1"><li>Internal table staging working<figure id="c031250a-f14c-418f-8758-73e93665e09f" class="image"><a href="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image20.png"><img style="width:700px" src="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image20.png"/></a></figure></li></ol><ol type="a" id="4d965672-8f65-479f-b5c7-ab1354ea03ba" class="numbered-list" start="2"><li>Here the stages act like extraction</li></ol><ol type="a" id="e4a64f06-7b5f-4319-adea-c5b93c4c6f09" class="numbered-list" start="3"><li>Use <em>put</em> command to put the table into the staging table.</li></ol><ol type="a" id="2b088f5a-c060-4ae7-852e-d23643b77c2b" class="numbered-list" start="4"><li>Then we load the data into snowfalke warehoues using the <em>COPY INTO</em> injestion service</li></ol><ol type="a" id="67f60e9a-898d-4dad-b094-8a57d128ea3e" class="numbered-list" start="5"><li>Steps to create stage and load data<ol type="i" id="9ca5b982-39de-467f-a971-c4eeb3bda1e3" class="numbered-list" start="1"><li>create stage:<ol type="1" id="f63af966-6c0b-4e6c-9791-8359b7301469" class="numbered-list" start="1"><li><em>create stage product_stage;</em></li></ol></li></ol><ol type="i" id="e7442b68-1dba-4928-9f6f-ef773556366d" class="numbered-list" start="2"><li>put file in stage:<ol type="1" id="0a7cdbf8-6c72-4dc2-9cca-fede6ec9bf37" class="numbered-list" start="1"><li><em>put file://C:\Users\Futurense\product_data.csv @product_stage;</em></li></ol></li></ol><ol type="i" id="a3d272a5-f2eb-4124-bcc3-8cee1278e207" class="numbered-list" start="3"><li>create table in snowflake database</li></ol><ol type="i" id="a5cbee6a-27f2-40db-ad6c-7fe1313f0e76" class="numbered-list" start="4"><li>copy data into table:<ol type="1" id="b7e7189e-2107-4d85-bd26-9c55cd3c6519" class="numbered-list" start="1"><li><em>copy into products from @product_stage/product_data.csv file_format=(type=&#x27;csv&#x27;,skip_header=1);</em></li></ol></li></ol><ol type="i" id="6eeb524f-2d3f-4bb3-81f9-2947dd8c7528" class="numbered-list" start="5"><li>After the completion of the loading we need to delete the data otherwise we will be charged money for the staging data also<ol type="1" id="eed7b5aa-9ba9-402b-9b1b-564299384aef" class="numbered-list" start="1"><li><em>drop stage product_stage</em>;</li></ol></li></ol><ol type="i" id="3804ffe5-a0cd-43d6-bc6d-3e3ee91bd002" class="numbered-list" start="6"><li>Copying data from snowflake to local<ol type="1" id="1f8b9ca1-7a5d-4f33-b6ed-323fc66f1a15" class="numbered-list" start="1"><li>create stage op_stage;</li></ol><ol type="1" id="e5128c0e-c8c1-4626-ba54-695acc5621bd" class="numbered-list" start="2"><li>copy into @op_stage/op1 from (select * from products where product_id = 1);</li></ol><ol type="1" id="7b144afc-61e0-4a8e-8425-a90a6d4df921" class="numbered-list" start="3"><li>list op_stage;</li></ol><ol type="1" id="2184b16b-a07a-4f6f-80df-f00eec0a3ef4" class="numbered-list" start="4"><li>get @op_stage/op1_0_0_0.csv.gz <a href="file://d/">file://D</a>:;</li></ol></li></ol></li></ol></li></ol><ol type="1" id="eb30bcec-8dc4-48f9-85d9-ac640dd68d76" class="numbered-list" start="2"><li>External stage<ol type="a" id="b9af2f6d-a302-4a54-ba12-46a86efbe888" class="numbered-list" start="1"><li>We create external stages to load data from cloud.</li></ol><ol type="a" id="66d39625-bb8d-46a5-b143-544ecf51ec9a" class="numbered-list" start="2"><li>for AWS open the pdf: <a href="https://www.notion.sofile:///C:/Users/Futurense/Futurense%20Training/BigData/snowflake%20external%20stage.pdf">snowflake external stage.pdf</a></li></ol><ol type="a" id="bb705b7b-963e-41af-a1a2-5e13056ad6f1" class="numbered-list" start="3"><li>For Azure: C:\Users\Futurense\Futurense Training\BigData\ Azure BLOB Storage to snowflake.word<ol type="i" id="70113bc6-a64a-4181-82a1-00b7b49bf569" class="numbered-list" start="1"><li>External stage</li></ol><ol type="i" id="3db849f3-739b-4021-83eb-50a2afd25892" class="numbered-list" start="2"><li>Snowpipe (Ingestion service)</li></ol><ol type="i" id="fe8e7205-8dd6-4810-9dba-8574adb50b46" class="numbered-list" start="3"><li>Unloading</li></ol><ol type="i" id="a8bb0794-08db-415b-9dfd-ffde3664f0e9" class="numbered-list" start="4"><li>Data Sharing</li></ol><ol type="i" id="d63ce88f-802a-4959-bd07-c6a9f2abef8a" class="numbered-list" start="5"><li>Snowflake integration with PowerBI</li></ol><ol type="i" id="25726159-bcb2-4ea2-b191-2502c9177476" class="numbered-list" start="6"><li>Time travel</li></ol></li></ol></li></ol><p id="9a698ad2-5919-4105-a9b6-d3c65f01a99b" class="">Azure storage(Blob storage) -&gt; Integration -&gt; External Stage -&gt; Copy Into -&gt; snowflake warehouse -&gt; client db -&gt; BI -&gt; report.</p><p id="0bc50939-2608-4fc4-ad62-32a3b5fba32c" class="">Azure blob storage and AWS S3 are not distributed file systems. They are object storages. This object storage is not suitable for analysis purpose. This object storage is suitable to only store the data. When you want to do analysis, bring the data into ADLS(Azure data lake storage) or AWS data lake.</p><p id="442df7c8-d049-445e-9252-0d3307fb3d1e" class="">Snowpipe:</p><p id="4396fcf2-fef7-4b28-be8c-0455f3433e47" class="">For importing data.</p><p id="55458dc4-e2f5-45d5-9c3a-0b5fa16ef0e2" class="">Pdf: <a href="https://www.notion.sofile:///C:/Users/Futurense/Futurense%20Training/BigData/Snowpipe.pdf">Snowpipe.pdf</a></p><table id="47d7937a-ce5f-45bf-8535-cc1e874932b8" class="simple-table"><tbody><tr id="1e99d459-efc4-449f-8571-a75905bf0cf0"><td id="{U|g" class="">Copy Into</td><td id="QqNa" class="">Snowpipe</td></tr><tr id="ec7e8c5c-b566-4c8c-afff-04d02cc677ea"><td id="{U|g" class="">1. No automation.</td><td id="QqNa" class="">1. Automation.</td></tr><tr id="d3fe903a-01f0-4488-9bfa-54a3fb1d566f"><td id="{U|g" class="">1. Need to run pipeline again if new data comes.</td><td id="QqNa" class="">1. New data is brought automatically.</td></tr></tbody></table><figure id="8d27608b-5eb8-4552-87b6-9d16db513020" class="image"><a href="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image21.jpeg"><img style="width:700px" src="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image21.jpeg"/></a></figure><p id="6f026251-7a8d-4dfe-889c-21768e6c7d4c" class="">Snowpipe Diagram</p><p id="72bb067b-14db-4392-8eec-75b221162e62" class=""><strong>Snowflake with PowerBI</strong></p><p id="0bc81645-414f-4b4c-8d1f-7d97078a4a89" class="">step 1: open PowerBi</p><p id="1f08f2fd-6f4f-4ee5-88a0-e8ed15579099" class="">step2: select get</p><p id="d343bd78-10f8-4a4a-8f8a-f14dd0b88f1a" class="">step 3: search for snowflake</p><p id="87acc763-f36c-4606-92da-40c561b3d1b7" class="">step 4: enter credentials</p><p id="4ebeefbe-ec96-4a3d-a89c-b05c03e0f418" class="">step 5: select the table</p><p id="ce0ea7c2-0459-43b2-9744-e1f784fa72d3" class=""><strong>Snowflake time travel</strong>:</p><ol type="1" id="08377731-161c-467b-8c4d-d4c4ff7b0d99" class="numbered-list" start="1"><li>it used to access the historical data at any point within a defined period. Time travel is used to analyze the data meaning that backing up data from keypoints in the past.</li></ol><ol type="1" id="5c8d831a-379e-4de3-9334-ddd8f511933d" class="numbered-list" start="2"><li>Using time travel, you can query the data in the past (updated data or deleted data). There are two functions used for the time travel data analysis. 1. at 2. before</li></ol><ol type="1" id="5ab8acec-f4cf-4c23-a76d-f3e6f926579f" class="numbered-list" start="3"><li>Example Time travel query using to select the historical data from the tables as of the date and time represented by timestamp. Using time travel you can retrieve the historical data from the table as of 30 minutes ago</li></ol><ol type="1" id="f979ab98-6009-4a1e-bb70-de8ada98f4f7" class="numbered-list" start="4"><li><em>select * from details at(offset=&gt; -10*60);</em></li></ol><ol type="1" id="490bb8d8-df9e-46af-8214-8bf27a9c9585" class="numbered-list" start="5"><li><em>select * from details at(timestamp =&gt; &#x27;Fri, 15 Sep 2023 16:20:00 +0530&#x27;::timestamp_tz);</em></li></ol><figure id="8ff9e8cc-2878-4892-ba55-7d4d2a998bf3" class="image"><a href="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image22.png"><img style="width:700px" src="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image22.png"/></a></figure><p id="67446226-2fa2-4435-9af2-b8948a7bc06e" class=""><strong>4 Tier Architecture Of Snowflake Data Engineering</strong></p><p id="6700bff5-fdc0-4cc4-9e9b-4e87ecf84e3c" class=""><strong>Snowflake Commands</strong></p><ol type="1" id="0ffcd4c2-2e95-4224-84d4-6fdf81f206d7" class="numbered-list" start="1"><li>create warehouse identifier(&#x27;&quot;training&quot;&#x27;) comment = &#x27;&#x27; warehouse_size = &#x27;x-small&#x27; auto_resume = true auto_suspend = 300 enable_query_acceleration = false warehouse_type = &#x27;standard&#x27; min_cluster_count = 1 max_cluster_count = 1 scaling_policy = &#x27;standard&#x27;</li></ol><ol type="1" id="43527ebc-12ff-47a6-8814-a7d9a18a0ce9" class="numbered-list" start="2"><li>drop warehouse training;</li></ol><ol type="1" id="ecb7979a-749e-42f0-80cc-b95d4045d017" class="numbered-list" start="3"><li>create stage stage_name;</li></ol><ol type="1" id="ac89b345-ba4c-4d38-806c-3f19c1666cf2" class="numbered-list" start="4"><li>drop stage stage_name;</li></ol><ol type="1" id="b6afd307-faeb-47f3-8d05-18dee6e65a7c" class="numbered-list" start="5"><li>Removes the files inside the stage but not the stage: rm @stage_name;</li></ol><ol type="1" id="9eb4992a-ce8c-48bd-bb43-31dc9644557a" class="numbered-list" start="6"><li>Overwrite the already existing file in stage: copy into @op_stage/op1 from (select * from products where product_id = 1) overwrite = true;</li></ol><p id="118020ed-a645-4f9a-88fa-fa4118e34c98" class=""><strong>Airflow</strong></p><p id="55d53fa7-88cc-4ec4-935c-82d4dddb4114" class="">Oozie It is a part of Hadoop eco sysyem, shell scripting, scheduling, not suitable for multinode cluster.</p><p id="33fed265-be84-4e77-8524-9b3ab3cd95f5" class="">Apache NIFi Scheduling with all components, graph based, not suitable for multinode cluster.</p><figure id="ba94d34a-22f8-408e-a469-60fe6660a19b" class="image"><a href="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image23.jpeg"><img style="width:700px" src="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image23.jpeg"/></a></figure><ol type="1" id="181a7096-0d9d-42ae-9ffc-1686241ccb88" class="numbered-list" start="1"><li>Airflow allows you to scheudle, monitor data pipelines and workflows (tasks or jobs) using web interface.</li></ol><ol type="1" id="fdfbf6f2-9ac5-4656-8425-f6d65a868da6" class="numbered-list" start="2"><li>It is open souce</li></ol><ol type="1" id="1034f1b6-b5ea-455c-bc41-82bcde545342" class="numbered-list" start="3"><li>It can be used with any services like DBs,cloud etc</li></ol><ol type="1" id="585ad5a2-fa62-4577-98c2-2b01bce7d348" class="numbered-list" start="4"><li>User friendly</li></ol><ol type="1" id="716b739c-3d31-4092-aaf9-69a8d087b051" class="numbered-list" start="5"><li>It will not create ETL pipelines, it will just schedule the pipelines to get executed.</li></ol><ol type="1" id="8efa4081-d51b-49ac-9d87-281b93f877ba" class="numbered-list" start="6"><li>Everything is drag and drop in airflow, sometimes you write code in python.</li></ol><ol type="1" id="7915fbfb-1f03-4c25-8b95-d03e09f7c9a3" class="numbered-list" start="7"><li>Executor is JVM and executes the actual task, it is assigned some processor cores and RAM.</li></ol><ol type="1" id="15640f82-fa83-4403-bb0f-b4899484dee8" class="numbered-list" start="8"><li>DAG directory stores all the details of airflow jobs. DAG means directed acyclic graphs, it shows where the job is started and where it is ended graphically. We can see the timings of different phases of execution of a job in DAG directory.</li></ol><ol type="1" id="3b830d02-bc72-4d23-bf63-ddf555094fed" class="numbered-list" start="9"><li>One Airflow job has one DAG. The job is known also known as one workflow.</li></ol><ol type="1" id="43987ed1-1286-4077-88fe-dd5e3439c2af" class="numbered-list" start="10"><li>We should create workflow using python scripts, we can write the script to execute the job immediately or schedule the job on basis of time or we can trigger the workflow based events.</li></ol><ol type="1" id="a8c19a81-6c12-4f71-a02a-e367f2c2599b" class="numbered-list" start="11"><li>We can write the script to retry the execution of the job incase of failures.</li></ol><p id="51588b17-55d5-44c9-8733-60b12306edd2" class="">DAG</p><ol type="1" id="be3ad467-eaca-49d1-a1f4-146edc8896f2" class="numbered-list" start="1"><li>DAG (Directed acyclic graph): It is a collection of tasks and describes how to run a workflow written in Python.</li></ol><ol type="1" id="b4b0ea97-6313-402d-ae1a-eb354705d627" class="numbered-list" start="2"><li>Pipelines are designed as a DAG by dividing the pipeline into tasks that can be executed independently.</li></ol><ol type="1" id="83fb1401-1eef-4fef-bf7c-97281e8cd155" class="numbered-list" start="3"><li>Then these tasks are combined logially as a graph.</li></ol><p id="abea6b34-63bd-4217-baed-61c89a5c04a1" class="">Task</p><ol type="1" id="ceb382e7-3ff8-4254-b357-22499d8d7721" class="numbered-list" start="1"><li>A unit of work within a DAG</li></ol><ol type="1" id="b46446d4-027a-41e7-8bc6-d5721d6385a8" class="numbered-list" start="2"><li>Hers tasks will be imlementing using operators.</li></ol><ol type="1" id="aa9daa6c-c733-4b7f-af97-efc34f7a5f6a" class="numbered-list" start="3"><li>Example of operators: Python operator, Bash operator (running bash commands), Postgre operators, S3 operator, sensor operators. These operators are using in a DAG to describe the task in pipeline.</li></ol><ol type="1" id="49ac8400-56b4-400c-9e78-1863c7a47944" class="numbered-list" start="4"><li>A standard template of the DAG to create workflow or a job.<ol type="a" id="5c9aa0a9-27a3-42fb-9a62-ebefd9e1f70a" class="numbered-list" start="1"><li>import modules and packages</li></ol><ol type="a" id="73beb2f4-f3a0-4a3b-9cc4-29403438c5a4" class="numbered-list" start="2"><li>define default arguments</li></ol><ol type="a" id="b99c629c-fe56-4c8c-ac11-9e9e6858841d" class="numbered-list" start="3"><li>Initiate the DAG (Dag object)</li></ol><ol type="a" id="2e64c6d6-5dec-417b-b275-348016c3c4a1" class="numbered-list" start="4"><li>Define the tasks</li></ol><ol type="a" id="6f18fa48-f506-4160-bde6-2fbbcbb2a355" class="numbered-list" start="5"><li>Define the dependency</li></ol></li></ol><p id="26410378-80c6-4aea-8a36-9090a3c4d3c8" class=""><strong>Spark</strong></p><p id="b4826044-ba6f-4ebe-afc5-30a3b2c9c86d" class=""><a href="https://onedrive.live.com/redir?resid=5EE50019F2C7B36D%211448&amp;authkey=%21AmB2QGT-RBWSMoA&amp;page=View&amp;wd=target%28Big%20Data.one%7C3363c0f0-5fe5-4127-937a-5ef5d77ab400%2FDate%20-%2007%5C%2F07%5C%2F2023%7C864f3645-de60-44b8-a2d9-04c691eca8c3%2F%29&amp;wdorigin=NavigationUrl">Koushik Notes</a></p><ol type="1" id="170007e3-feec-42fb-af16-4ebbad4d75e1" class="numbered-list" start="1"><li>Spark is an opensource cluster computing framework.</li></ol><ol type="1" id="0141197d-6e2a-486f-99fa-d8493be7ab4e" class="numbered-list" start="2"><li>It is for processing large scale data (not for data storage)</li></ol><ol type="1" id="4d471b40-e559-4f79-bab2-c3884cbfee1c" class="numbered-list" start="3"><li>Spark can read data from a cluster(Like HDFS)</li></ol><ol type="1" id="d78c4d56-a052-46af-ac0d-a76c41d32ddb" class="numbered-list" start="4"><li>It is built using scala (scala is built using Java)</li></ol><ol type="1" id="55c58287-a3b2-416b-b32d-73c977b97dab" class="numbered-list" start="5"><li>It runs the processing in memory only. Not in disk.</li></ol><ol type="1" id="bdc7da47-5150-47fa-8e9c-fd8528b6cd87" class="numbered-list" start="6"><li>Spark is 100 times faster in memory and 10 times faster in disk as compared to MapReduce.</li></ol><ol type="1" id="86cd5149-aebd-4d0e-a7b8-9ef15d834de1" class="numbered-list" start="7"><li>Spark reads data from everywhere like HDFS, Mesos, Standalone cluster, data sources (AWS S3, azure blob)</li></ol><ol type="1" id="942b5af3-72a3-4790-9d26-cf52e92153be" class="numbered-list" start="8"><li>Spark is a unified platform because it provides data frames, sql, complex programming, ML, streaming (for real time data processing).</li></ol><ol type="1" id="85bb1da5-6c3f-4907-960c-acac2f9173c7" class="numbered-list" start="9"><li>Master slave architecture</li></ol><ol type="1" id="d66aaf31-983e-489f-8317-f7a401423295" class="numbered-list" start="10"><li>Spark use scala, scala uses lazy evaluation. (not do the work until it is required).</li></ol><ol type="1" id="e2490b1f-8a6f-4b67-bce7-f4e4b2d2dbb9" class="numbered-list" start="11"><li><figure id="77fa08f7-ce64-40ed-8797-3e12088c56b4" class="image"><a href="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image24.png"><img style="width:526px" src="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image24.png"/></a></figure><ol type="1" id="455073d6-af93-479e-b440-1f08c7d68ae9" class="numbered-list" start="1"><li>Architecture in terms of Execution process.</li></ol><ol type="1" id="c71db63c-4999-49a8-8157-695e213ad39f" class="numbered-list" start="2"><li>In JVM there are Executors. Within that executors we have threads available.</li></ol></li></ol><p id="09b5c2c7-a9e0-4e95-870f-6139cc667412" class=""><strong>RDD</strong>:</p><p id="41f1edc0-c99a-4f17-945b-c534f2ec15c0" class="">Read only.</p><p id="79dd0512-f497-4c16-bed9-32ae73fe6fcf" class="">RDD resilient distributed datasets. It is the main abstraction of spark.</p><p id="7890bf92-36cb-483b-973a-5db625bc5da2" class="">Distributed collection of elements which are read only(immutable), partitioned (parallelism), runs in parallel, fault tolerant, lazy evaluation.</p><p id="c35dbda3-b9e6-4dcd-a7e3-2d233f891bdc" class="">A &quot;distributed collection of data elements&quot; refers to a dataset that is spread across multiple machines or nodes in a distributed computing environment. In this context, &quot;collection&quot; simply means a group or set of data elements, such as numbers, text, records, or any other data type.</p><p id="e881ff4e-edf6-40eb-a27d-2deacf972cd3" class=""><strong>Characteristics:</strong></p><ol type="1" id="89d2a1f3-6ded-47d2-b9fa-f2a027ea57ad" class="numbered-list" start="1"><li>Resilient (Fault tolerant)</li></ol><ol type="1" id="29df7c2d-e4f9-4f19-8948-2fd6f8796f3d" class="numbered-list" start="2"><li>Distributed</li></ol><ol type="1" id="3e28e5f0-7704-4d7b-a11b-1500bcba16f7" class="numbered-list" start="3"><li>Immutable</li></ol><ol type="1" id="b92bedc4-c5bb-4612-9a3d-a9a18bde0de8" class="numbered-list" start="4"><li>Lazy Evaluation</li></ol><ol type="1" id="b1cd3026-99bf-4d52-afd9-21eda9a77929" class="numbered-list" start="5"><li>Parallel Processing</li></ol><ol type="1" id="e4eae82a-f2cb-4c8e-b9de-01b8d999e649" class="numbered-list" start="6"><li>In-Memory Storage</li></ol><p id="6a7c79fe-db65-4a81-8cab-feb2ff0ff877" class="">Transformations The functions applied are called transformations. The work is not done yet.</p><p id="71823614-b15b-410c-a31e-7b95ac8e91de" class="">DAG Upon applying a transformation spark only creates a logical plan of execution (estimating computing required).</p><p id="d2073773-7ec6-4d37-8bfa-7183e11c16a9" class="">Action Upon calling an action, the actual execution will happen using the DAG reference.</p><p id="362d9f38-2b13-45e4-91a4-212cb51cf925" class=""><strong>Parallelization:</strong></p><p id="bc1688d7-ee38-496b-8182-32bda1376a77" class="">CPU</p><ul id="b35c494a-8f56-43fa-bbf0-e6b82e6a98c0" class="bulleted-list"><li style="list-style-type:disc">Core</li></ul><ul id="3cea8be0-0a6b-45b2-ac24-2159ecd156cf" class="bulleted-list"><li style="list-style-type:disc">Thread</li></ul><ul id="e30cfa3b-d9b4-4277-ab2b-a2a8408f6924" class="bulleted-list"><li style="list-style-type:disc">VCPU (Core in cloud)</li></ul><ul id="97abe6ed-98a5-41e1-b655-e10b1faad0e2" class="bulleted-list"><li style="list-style-type:disc">SLOT (Threads that are available to perform a task)</li></ul><p id="a126e60f-db9d-4697-830f-bd5e293db360" class="">When you use hyper v you can use 2 threads in a single core.</p><p id="a422fa67-0f65-4c0a-a015-b4974f63dc77" class="">With HyperV: 1 socket = 4 cores = 8 threads (8 VCPU in cloud)</p><p id="3bf76be3-8d8d-4792-89c0-d7df13fb7ee0" class="">Without HyperV: 1 socket = 4 cores = 4 threads (4VCPU in cloud)</p><p id="2ded10ab-12db-4cde-9f0a-b9d7e1a721b8" class="">Vertical scaling: Increasing the resources in the computer.</p><p id="eb5ee266-572f-403b-87fb-735e920871d2" class="">Horizantal Scaling: Increasing the number of computers.</p><p id="0086f4a7-dabb-49e9-8902-9a4d48f09c5c" class="">One Partition will have one task. By default the number of partitions of an RDD = number of cores.</p><p id="880edb47-bd90-4fae-a5a3-584f98466929" class=""><strong>Architecture:</strong></p><figure id="7f6986bf-b096-45ae-866b-aa313b2cefa7" class="image"><a href="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image25.jpeg"><img style="width:700px" src="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image25.jpeg"/></a></figure><p id="35227b28-7d10-4dfe-acf4-521b04a79505" class="">The amount of data that fits into the ram it processes the data in memory, after that it will go to the other parts of the data. It is called spill.</p><p id="d580c716-f2cd-422d-8253-80bb345d61c6" class="">Spark needs a resouce manager it could be YARN, Mesos or stand alone.</p><p id="d3be6b39-d0d4-4c24-981c-97a0c92f1845" class="">The core API of spark is RDD.</p><p id="38ad8773-dbb2-4e73-8b58-afd4fb8a60bc" class=""><strong>Spark Session:</strong></p><p id="c41c4c76-4b8b-48b7-9cad-9aa4c9ce472c" class="">You need to register in order to use.</p><p id="3fef3496-fa1a-4bdc-a52c-e8dd00c30118" class="">There are other sessions like spark context in the earlier versions. They are: SQL context, Hive context, Streaming Context. But now there is no need to use all these contexts seperately. We can directly use the Spark Session to have all these contexts.</p><p id="1696b941-6567-4d9c-8aa6-788469d3ef3c" class=""><strong>Transformations:</strong></p><ol type="1" id="1cd6fd9c-2226-470e-a5b2-fc7237f0215d" class="numbered-list" start="1"><li>Narrow transformation (does on one data value)<ol type="a" id="d424ae8b-343e-4bc3-8986-5ddf7c7ffb02" class="numbered-list" start="1"><li>They are all put in one stage</li></ol><ol type="a" id="9a32a6e1-112e-47cd-8951-90d7a403cb4d" class="numbered-list" start="2"><li>Eg: map, filter, mapPartitions</li></ol></li></ol><ol type="1" id="ba014ef7-2563-4f3d-b4c9-c5d26abe7256" class="numbered-list" start="2"><li>Wide transformation(does on multiple data values like group by)<ol type="a" id="9dad041c-1fe6-4f9d-bddb-589c372ce9d9" class="numbered-list" start="1"><li>They involve in multiple stages</li></ol><ol type="a" id="2e11b16b-1cdb-4346-9bce-e7ef52444908" class="numbered-list" start="2"><li>Eg: reduceByKey, groupByKey, joins, repartitions</li></ol></li></ol><p id="ecc314ee-1c31-4adf-93ff-4157a65da433" class=""><strong>Joins</strong></p><ol type="1" id="5de2007f-abb2-40b4-8c53-b66e56bc1bfd" class="numbered-list" start="1"><li>safe join<ol type="a" id="f2ed0b7f-eb31-45b8-9f84-8c718449f05f" class="numbered-list" start="1"><li>sort merge, uses shuffling, use when both the files are large</li></ol><ol type="a" id="cdedb71c-35f0-45b5-a0ba-608d8a383910" class="numbered-list" start="2"><li>broadcast join: copy the smaller file in every node as read only and then join it with the original tables.</li></ol></li></ol><p id="bdc9248a-2dba-41f8-8d32-fb74a7d762df" class=""><strong>Shared variables</strong></p><ol type="1" id="0f589e9a-0ff2-485d-863b-80bbac64c8b7" class="numbered-list" start="1"><li>Accumulators<ol type="a" id="8c29eed3-5de8-4665-b8b0-dcfb276d7123" class="numbered-list" start="1"><li>Implements counter</li></ol></li></ol><ol type="1" id="5ca672c7-0fd7-4dd6-a1fb-3ebe090530bf" class="numbered-list" start="2"><li>Broadcast variables<ol type="a" id="af6b6ae7-0f33-47d6-88ff-c88c6db9447e" class="numbered-list" start="1"><li>Implement read only data from the smaller file to the larger file</li></ol></li></ol><p id="bd6a93f7-a0ee-4d8f-9a6c-2123d2967111" class=""><strong>DataFrames</strong></p><ol type="1" id="26f38447-c86b-486c-b440-0be92d1e8d67" class="numbered-list" start="1"><li>Internally dataframe is stored as row objects.</li></ol><p id="e703d441-2104-4481-8d4b-a5b914e52377" class="">RDD vs Data Frame vs Data sets:</p><p id="723ec642-f6a5-4a44-bea1-8a946051f0cb" class="">DataFrame: distributed collection of row objects</p><p id="13446bfc-e173-45ec-a488-034b55160714" class="">RDD vs DataFrame vs DataSet</p><table id="41293426-4e49-44e3-8b9b-640bb276417b" class="simple-table"><tbody><tr id="44c067fc-f367-4e77-b68e-7b2769e0687f"><td id="x_?S" class=""><strong>RDD</strong></td><td id="NRMq" class=""><strong>DataFrame</strong></td><td id="H[Gr" class=""><strong>DataSets</strong></td></tr><tr id="d314c0ed-fc19-4522-8ec5-2dd9abf27e03"><td id="x_?S" class="">Distributed Collection</td><td id="NRMq" class="">Distributed Collection of Row Objects.</td><td id="H[Gr" class=""></td></tr><tr id="a3b75a75-d9c4-4d47-bd1d-eddde5efa1a7"><td id="x_?S" class="">Immutable</td><td id="NRMq" class="">Infer schema, it gives structure of data</td><td id="H[Gr" class=""></td></tr><tr id="fbc4b892-8c24-4a13-ae70-c6e139d1abc0"><td id="x_?S" class="">Fault Tolerance</td><td id="NRMq" class="">Hive,SQL Compatibility</td><td id="H[Gr" class=""></td></tr><tr id="b61977f1-5e0b-42a3-a829-ea2053cf9571"><td id="x_?S" class="">Lazy evaluations</td><td id="NRMq" class="">Lazy evaluation</td><td id="H[Gr" class="">Lazy evaluation</td></tr><tr id="18086a02-57de-4f49-a53a-bc85e97e47d7"><td id="x_?S" class="">Unstructured Data<br/><br/>Doesn’t infer schema, optimization of code is manual</td><td id="NRMq" class="">optimization → Catalyst Optimiser (Tungsten)</td><td id="H[Gr" class=""></td></tr><tr id="14500e6d-c01b-4c90-b738-d1f05ce0fcb8"><td id="x_?S" class="">Scala, Java, R, Python</td><td id="NRMq" class="">Available in Java, Scala, Python, R</td><td id="H[Gr" class="">Datasets are available only for Java and scala</td></tr><tr id="2949ee20-264a-46e1-a026-2a379a3a71ac"><td id="x_?S" class="">It is compile safe. i.e., if error is present then it tells in the compile time only.</td><td id="NRMq" class="">It throws error only when you run not when you compile.</td><td id="H[Gr" class=""></td></tr><tr id="6c1af46a-5597-4a19-a761-fb915fe5439e"><td id="x_?S" class="">Advantages: OOPs programming, type safety</td><td id="NRMq" class="">Relational format, optimization</td><td id="H[Gr" class="">Both the advantages of RDD and DF.</td></tr><tr id="d2963819-a6a0-45d9-9ba1-8750a83c152c"><td id="x_?S" class="">Suitable for smaller datasets as it stores data in onHeap.</td><td id="NRMq" class="">Suitable for large dataset, it uses offHeap.</td><td id="H[Gr" class=""></td></tr></tbody></table><figure id="84f830a1-5f3c-4f61-afd2-6630bd1ae8e2" class="image"><a href="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image26.png"><img style="width:700px" src="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image26.png"/></a></figure><p id="c525a1fa-02f0-4a4f-afbe-01d43370d7f5" class="">On heap memory:</p><ol type="1" id="6fb726cb-16fd-4f1c-b56f-7bbac7790c73" class="numbered-list" start="1"><li>Serialization and de serialization is needed.</li></ol><ol type="1" id="f5d011e1-c9ee-40b1-a5f5-3b219bfedc36" class="numbered-list" start="2"><li>It is present within the executors.</li></ol><ol type="1" id="4e677348-4ad4-428d-8df2-b0b0d4b5d844" class="numbered-list" start="3"><li>It is controlled by JVM</li></ol><ol type="1" id="9c76c9d8-ef88-44de-96c5-786f7f5b5309" class="numbered-list" start="4"><li>Garbage collection happens</li></ol><p id="1d7b5afb-4bab-46ee-86c4-ba292a01f9f9" class="">Off heap memory:</p><ol type="1" id="45f9235d-84bf-4974-a1b9-773cee2131b3" class="numbered-list" start="1"><li>Serde is not needed.</li></ol><ol type="1" id="976c0c79-2226-4da5-9140-c4591a631809" class="numbered-list" start="2"><li>It is controlled by OS.</li></ol><ol type="1" id="d8bbb51c-7274-45ab-9f65-48d4869ac7f8" class="numbered-list" start="3"><li>spark.memory.offHeap.size</li></ol><ol type="1" id="37f9d992-a968-4dab-834d-dff4de06ef82" class="numbered-list" start="4"><li>No garbage collection</li></ol><table id="648b42b6-f6d1-4ab0-9c49-95e90f64cca1" class="simple-table"><tbody><tr id="f66957f8-9948-4e82-9a85-630283287db7"><td id="~be]" class="">On Heap</td><td id="?DiZ" class="">Off heap</td></tr><tr id="24356f95-1e89-44d2-8269-578db1bcab54"><td id="~be]" class="">1. Better perfrormance, bcoz object allocation and deallocation is automatic</td><td id="?DiZ" class="">1. Slower than onHeap, but better than disc.</td></tr><tr id="4b3651e7-414a-4d59-bb96-55ebee4bb60b"><td id="~be]" class="">1. Manged and controlled by garbage collector.</td><td id="?DiZ" class="">1. Managed by OS no need of GC</td></tr><tr id="97673c9f-920b-4f57-a03a-5ef12f4f81ab"><td id="~be]" class="">1. Data stored in form of java bytes(serialized)</td><td id="?DiZ" class="">1. In form of array. No overhead of ser and de ser.</td></tr><tr id="d30e3ae4-fc7f-42e3-807a-124cf0aeed5a"><td id="~be]" class="">1. Suitable for smaller sets of data</td><td id="?DiZ" class="">1. for bigger data</td></tr></tbody></table><p id="4aca4713-86a2-43fd-a493-c404b1e5f4a9" class="">Unified memory: It has executor memory and storage memory.</p><p id="53a2c8d7-2d07-451c-ba95-b206adda2387" class=""><strong>Data Validation modes in DataBricks DF:</strong></p><p id="3953d33a-82ca-49d5-ac6e-93f671a20d7d" class="">There are 3 types of Data validation rules they are</p><p id="b2b6f9c3-16d0-4641-9657-276ac63e3ba3" class="">df=spark.read.format(&quot;csv&quot;).option(&#x27;sep&#x27;,&quot;,&quot;).option(&#x27;header&#x27;,False).option(&#x27;mode&#x27;,&#x27;permissive&#x27;).schema(df_schema).load(&quot;dbfs:/FileStore/tables/mydata/currupt_data.csv&quot;)</p><ol type="1" id="3b838ff3-8259-47e9-9d75-f75c0a6681d6" class="numbered-list" start="1"><li>Permissive: Allow all the records in the new column.</li></ol><ol type="1" id="d0f64188-52e6-4bc7-b004-f54295d2913a" class="numbered-list" start="2"><li>DropMalforced: Drop the corrupted records.</li></ol><ol type="1" id="e8bc781c-64b9-48ec-985e-2fc84e930075" class="numbered-list" start="3"><li>FailFast: Fail the importing process if there are corrupt records.</li></ol><p id="3c570362-6d77-4f6d-a02f-b67bb8cb5c8d" class=""><strong>Catalyst optmizer</strong></p><p id="08f73675-e3d7-48ae-bdd1-b70dafdd0c25" class="">Cost based optimizer: Based on the statistics cost is estimated for diffrenet approaches of the executions and the best is used.</p><p id="4e4b757b-8c77-412c-95bb-d4486eae7bfb" class="">Rule based optimizer: Set of predefined rules to design the execution plan.</p><figure id="13e4fe69-b79d-49ef-94be-179af88f12a3" class="image"><a href="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image27.png"><img style="width:700px" src="Big%20Data%20Notes%20d8626f869657445e837ee1a150d493f9/image27.png"/></a></figure><p id="4985e609-7d21-4006-9e21-d5c9ca7f50b8" class="">Dynamic coalescing shuffle partitions.(Avoids the difficulty in selecting the correct number of partitions)</p><p id="c2eef413-f606-4c60-a7b3-a63c292918f8" class="">Dynamic switching of join strategies.</p><p id="a370ac39-010a-46d4-b444-bc81bff6e53f" class="">Dynamically optmizing the skew joins.</p><p id="df7dc532-6607-4360-a17e-a4123a26ca34" class=""><strong>Adaptive Query Optmization</strong></p><p id="55757122-9c6d-49d2-930b-20d403447615" class="">In Spark 3.0, Adaptive Query Execution (AQE) was introduced. One difference between AQE and Catalyst Optimizer is that AQE modifies the Physical Plan based on Runtime Statistics, so AQE can tune your queries further on the flight.</p><p id="bab5dff7-6d08-45ea-ad91-4e526c0c81e4" class=""><strong>Spark Streaming</strong></p><p id="6c5e8e41-b4f5-4047-a7ca-f6c2163c4c6f" class="">Spark has a library called Spark Streaming. It enables scalable, high-throughput, fault-tolerant stream processing of live streaming data.</p><p id="de20179b-5599-4ed3-acbd-9f41e5b7268b" class="">Two categories of streaming: Basic and advanced.</p><p id="fe168a93-5c86-48d6-bcb0-470c48739cd0" class="">Basic sources: File systems and socket connections</p><p id="a1579dd0-9543-41c8-b6b6-d227fcdc3c67" class="">Advanced sources: Kafka, Flume, Twitter etc</p><p id="aada1085-c29e-4d43-848c-3863f5c4461d" class="">There will be a thread for handling inputting of data, and then we should have another thread to process the data.</p><p id="3d42a6e5-2b15-4a32-8570-00f730b600e3" class="">Dstreams: It reperesents a stream of data that comes continually.</p><p id="14859ac7-f7e8-4787-96a5-f64a90fe7ed3" class=""><strong>KAFKA:</strong></p><p id="d0df2240-9dc8-470d-8745-7ff4d581409a" class=""><strong>Functions:</strong></p><ol type="1" id="efd46440-1c86-49f3-b0e4-a319ecad6cba" class="numbered-list" start="1"><li>Create context:<ol type="a" id="35c683fb-2188-44df-9f7c-65189e8d97c7" class="numbered-list" start="1"><li>sc.getOrCreate()</li></ol></li></ol><ol type="1" id="ae6d54ad-fcb5-4be9-b0aa-5f962cedfbb3" class="numbered-list" start="2"><li>Create RDD:<ol type="a" id="4d16fa7b-78a5-45fd-8de5-e322fe66965c" class="numbered-list" start="1"><li>sc.parallelize([1,2,3,4])</li></ol></li></ol><ol type="1" id="0fb5ddfb-f3c6-4841-b987-85f029e24702" class="numbered-list" start="3"><li>Read text file as RDD:<ol type="a" id="f722c3bd-b8f3-4fc6-8c71-c8d09eb1dfb2" class="numbered-list" start="1"><li>sc.textFile(&#x27;file_path’, num_of_partitions)</li></ol></li></ol><ol type="1" id="88c807f4-a62f-45ba-9408-41851d0a1540" class="numbered-list" start="4"><li>See the number of partitions made:<ol type="a" id="86acd4bd-d685-434c-83a5-58a4b51861df" class="numbered-list" start="1"><li>file. getNumPartitions()</li></ol></li></ol><ol type="1" id="bc847539-213f-4eaa-af9d-51ea3e94acda" class="numbered-list" start="5"><li>See the entire data:<ol type="a" id="c997a24f-a792-4834-a9d0-fe692a4c6d4d" class="numbered-list" start="1"><li>file .collect()</li></ol></li></ol><ol type="1" id="82ef2444-e4d1-4a35-88f9-a4361e65d20e" class="numbered-list" start="6"><li>Count the number of lines:<ol type="a" id="85f6e0f6-f440-491f-bfb2-21960a49cf3c" class="numbered-list" start="1"><li>file.count()</li></ol></li></ol><ol type="1" id="509daa59-2c3d-4fec-96a6-a2b7c7167172" class="numbered-list" start="7"><li>Read the data partition wise:<ol type="a" id="3689bf2a-09f0-4ac3-b430-cbb284b1a268" class="numbered-list" start="1"><li>file.glom().collect()</li></ol></li></ol><ol type="1" id="658cb441-4592-498a-ac0a-9ab6d028ca5a" class="numbered-list" start="8"><li>Read specific number of lines:<ol type="a" id="10195bf7-9ded-4b80-b810-e103cc829cbb" class="numbered-list" start="1"><li>file.take(num_of_lines)</li></ol></li></ol><ol type="1" id="bfaed423-d0ca-43da-9464-3d17fdf26251" class="numbered-list" start="9"><li>Apply a function to a RDD having single partition:<ol type="a" id="e4d098a7-4259-4de0-a5f0-587c0ab9162d" class="numbered-list" start="1"><li>file.map(functoin)</li></ol></li></ol><ol type="1" id="e63bb3ec-9377-433b-b839-c5c2f7c542a7" class="numbered-list" start="10"><li>Apply a function on key value pair sequences( the function will be applied to values only)<ol type="a" id="0b34ee76-5113-4b26-9ecd-deadc33ddede" class="numbered-list" start="1"><li>file.mapValues(function)</li></ol></li></ol><ol type="1" id="0f171780-b7c1-44bd-9b82-a0c235a704ae" class="numbered-list" start="11"><li>Apply a function to a RDD partition seperately:<ol type="a" id="f076bd18-9980-4b62-b368-196a780254ac" class="numbered-list" start="1"><li>file.mapPartitions(function)</li></ol></li></ol><ol type="1" id="c2eacfab-7cf5-4ade-97c3-1bfb39fd1eeb" class="numbered-list" start="12"><li>Filter the data in RDD:<ol type="a" id="5f51467f-2ba4-4b69-bd53-dd5f2a4a0bec" class="numbered-list" start="1"><li>file. filter(function)</li></ol><ol type="a" id="790bc7be-c4ec-4ed2-93e2-fe119b771b6e" class="numbered-list" start="2"><li>Note that the function should return boolean</li></ol></li></ol><ol type="1" id="3f66a4e6-53ec-433f-b25c-6167aff02ada" class="numbered-list" start="13"><li>Get the top elements in an RDD:<ol type="a" id="90f88e74-7a8c-4f9a-8cea-d36b5cfbb2a6" class="numbered-list" start="1"><li>x_part.top(n)</li></ol></li></ol><ol type="1" id="25207955-efa9-484a-9e42-10ad23d5ea0b" class="numbered-list" start="14"><li>Get the max and min elements<ol type="a" id="2a6ef4d2-1330-4f7e-8883-c5da3599b558" class="numbered-list" start="1"><li>x.max()</li></ol><ol type="a" id="1ee37d78-9d47-433d-9269-0e94c4a0c0c9" class="numbered-list" start="2"><li>x.min()</li></ol></li></ol><ol type="1" id="984325dc-023a-4f2c-aaa9-70c61b6f530b" class="numbered-list" start="15"><li>Find the stats like count, mean, std, max, min<ol type="a" id="a249429e-b238-41c9-a9a0-b4ef28fe5e56" class="numbered-list" start="1"><li>x.stats()</li></ol></li></ol><ol type="1" id="f1626e56-b042-4f84-9c9c-cc40f125091a" class="numbered-list" start="16"><li>Get the data from an RDD as a dictionary<ol type="a" id="46dc1b33-f483-4917-8d97-084f104b90f0" class="numbered-list" start="1"><li>rdd.collectAsMap()</li></ol></li></ol><ol type="1" id="316eedd9-da90-45f3-b71b-5c73495fc7e9" class="numbered-list" start="17"><li>Reduce the number of partitions (Not the actual rdd is partitioned but it returns the reduced one)<ol type="a" id="450a2923-0fb7-4791-9154-8477b15ad400" class="numbered-list" start="1"><li>coalsesce<ol type="i" id="e3a0d5c9-6789-4e9a-9275-f4439d6da394" class="numbered-list" start="1"><li>x_part.coalesce(2).glom().collect()</li></ol></li></ol><ol type="a" id="86a3b6a5-b567-43df-9d1d-6045331969a9" class="numbered-list" start="2"><li>repartition<ol type="i" id="badce637-46ac-49c9-9621-5a19ff9ccc53" class="numbered-list" start="1"><li>x_part.repartition(2).glom().collect()</li></ol></li></ol></li></ol><ol type="1" id="d6c35a8b-8cdb-4a10-89b7-d2b35174d5af" class="numbered-list" start="18"><li>Get the default parallelism set<ol type="a" id="c84417f2-026b-4d36-b7a7-27f483239ee8" class="numbered-list" start="1"><li>sc.defaultParallelism</li></ol></li></ol><ol type="1" id="1b4fd261-eacb-44c0-a193-1602fdef0997" class="numbered-list" start="19"><li>Get the default min partitions alowed<ol type="a" id="f53c30d4-6fbc-4e23-a466-3d24dcc5c666" class="numbered-list" start="1"><li>sc.defaultMinPartitions</li></ol></li></ol><ol type="1" id="8e7b5737-1a76-4c9c-b1be-51c9d616949b" class="numbered-list" start="20"><li>To save the result of a transformation in user dfined storage level<ol type="a" id="efee23f2-ad49-4dd2-9624-7eb876a58698" class="numbered-list" start="1"><li>x_part.persist()</li></ol><ol type="a" id="28d8937c-c44e-4793-b06d-6eb5e5bb93f4" class="numbered-list" start="2"><li>The allowed memory locations are:<ol type="i" id="0ebd1d33-2e5f-458f-bd56-1d3bdfc7b8ae" class="numbered-list" start="1"><li>MEMORY_ONLY</li></ol><ol type="i" id="c404da38-dc03-407a-ab0f-045e0ce924d7" class="numbered-list" start="2"><li>MEMORY_AND_DISK</li></ol><ol type="i" id="2ba2a4c9-275d-44a7-a203-5cb64fe89c24" class="numbered-list" start="3"><li>MEMORY_ONLY_SER</li></ol><ol type="i" id="f47e95e0-6a38-4cc4-972c-0c6f2129835f" class="numbered-list" start="4"><li>MEMORY_AND_DISK_SER</li></ol><ol type="i" id="811c36fa-52cc-4ddf-a1fb-dab9518671eb" class="numbered-list" start="5"><li>DISK_ONLY</li></ol></li></ol><ol type="a" id="ba1c0de6-b42a-4eb7-a67f-c8678cb65bf3" class="numbered-list" start="3"><li>Once a result is stored in a location, it cannot be changed again. You can change the upcoming locations of the results but not the one which is already placed in a location.</li></ol></li></ol><ol type="1" id="70b86c0e-0d7c-4274-be81-be071a3315ed" class="numbered-list" start="21"><li>to save the result in memory<ol type="a" id="e2b2eaa3-d856-4b06-bb65-e882e16f9212" class="numbered-list" start="1"><li>x.cache()</li></ol></li></ol><ol type="1" id="a2e8bf1a-fb8d-476c-b208-da0acf16f683" class="numbered-list" start="22"><li>To remove the saved data from the memory<ol type="a" id="379e9f9e-5c52-4712-a465-126f1d450f4c" class="numbered-list" start="1"><li>x.unpersist()</li></ol><ol type="a" id="2b6e0584-be98-424c-8963-ee3a1b027868" class="numbered-list" start="2"><li>x.unpersist(blocking = True)</li></ol></li></ol><ol type="1" id="6e3d390d-aad9-4094-a60d-afd081e3c268" class="numbered-list" start="23"><li>get all the persisted resulsts:<ol type="a" id="19cc4b39-032f-4b35-bce7-8f2a1ed37427" class="numbered-list" start="1"><li>spark.sparkContext._jsc.getPersistentRDDs().items()</li></ol></li></ol><ol type="1" id="95ab9518-a8a8-4bf5-aebb-7737bd31a484" class="numbered-list" start="24"><li>To unpersist all the items:<ol type="a" id="3da7f2a1-4f44-4cef-8b3b-510220adfdcb" class="numbered-list" start="1"><li>use a loop and then unpersist the items</li></ol><ol type="a" id="0e68a5f4-d618-4a31-90e6-c0fd97fce0af" class="numbered-list" start="2"><li>for (id,rdd) in spark.sparkContext._jsc.getPersistentRDDs().items():</li></ol></li></ol><blockquote id="32a8782a-55e5-4122-ad26-72c984b3574a" class="">rdd.unpersist()</blockquote><p id="f221d6f4-d164-4396-bfc3-49d0da0550bb" class=""><strong>DataFrames</strong></p><ol type="1" id="9ed07e9c-4e03-4957-90bc-a03fc92075b0" class="numbered-list" start="1"><li>Set the user defined column data types in a dataframe:<ol type="a" id="b18ab53a-1388-440f-a7d5-ef021842d457" class="numbered-list" start="1"><li>Two things invovled StructField and StructType</li></ol><ol type="a" id="dbc4be60-0497-4bb2-a94f-00d1a56a528a" class="numbered-list" start="2"><li>StructField is to define the data type and name of the column, StructType is the list of StructFields</li></ol><ol type="a" id="de8c4c4a-9303-4ffc-9cbc-2ea48215a216" class="numbered-list" start="3"><li>StructField(col_name, data_type, True)</li></ol></li></ol><ol type="1" id="760c18ba-54a0-45fb-a1dd-5fd4906b4b03" class="numbered-list" start="2"><li>Change the column names of the dataframe<ol type="a" id="d06e6c1f-e35d-40a7-b98b-0f903c3a9ad1" class="numbered-list" start="1"><li>df.withColumnRenamed(‘old_col_name’, ‘new_col_name’)</li></ol></li></ol><ol type="1" id="e695a27e-dfb0-4f88-9bc9-839e984c0d17" class="numbered-list" start="3"><li>Create a dataframe from an RDD:<ol type="a" id="7a322440-e677-4326-9d3d-57b74ed49354" class="numbered-list" start="1"><li>rdd.createDataFrame(StructType(fields = [StructField(…),…]))</li></ol></li></ol><ol type="1" id="d8c7ee35-d2e8-4903-b730-c67fb2e48c7a" class="numbered-list" start="4"><li>Infer Schema from the file automatically while importing from a csv<ol type="a" id="a4632608-c401-43cb-9795-fcc6bff96bc6" class="numbered-list" start="1"><li>spark.read.csv(‘file’, inferSchema = True)</li></ol></li></ol><ol type="1" id="f4863473-e02a-459f-8120-a98abb162012" class="numbered-list" start="5"><li>Filter DataFrame records<ol type="a" id="fb1fd801-f58e-49e3-a8e7-439eb31398ac" class="numbered-list" start="1"><li>df.filter(df[‘age’] == 18).show()</li></ol></li></ol><ol type="1" id="88472966-5891-499c-aa16-22c1580d1dba" class="numbered-list" start="6"><li>dropna<ol type="a" id="420ce43a-a26c-40b7-8be0-2d85ea78cdf5" class="numbered-list" start="1"><li>df.dropna(thresh = 2)</li></ol><ol type="a" id="c2ab6073-178a-49c1-90c4-572f523cba37" class="numbered-list" start="2"><li>using thresh we can mention the number of allowed null values in a row</li></ol></li></ol><ol type="1" id="b9007f3c-7d48-4ed5-9c69-d56463dd18a4" class="numbered-list" start="7"><li>modify, create, perform calculation on a column<ol type="a" id="2fb05036-c73a-47a7-8979-32dba72eb414" class="numbered-list" start="1"><li>df.withColumn(‘new_col’, col)</li></ol><ol type="a" id="e4bdfb60-566a-44b7-b032-ab2d7bf2d64e" class="numbered-list" start="2"><li>Here the col is an expression or function that defines the new column or updated column.</li></ol></li></ol><ol type="1" id="05ccab2d-47fe-4143-b3d4-a7b3f43be233" class="numbered-list" start="8"><li>Create an SQL table<ol type="a" id="5df05b7b-a42d-43e3-9aa7-2c47a8d48602" class="numbered-list" start="1"><li>df.createOrReplaceTempView (‘SQL_table_name’)</li></ol></li></ol><ol type="1" id="27d8a76b-a09e-41ec-be47-819d7e390217" class="numbered-list" start="9"><li>Query an SQL table from pyspark<ol type="a" id="afd2676f-fcd7-426a-8cbc-1bbdcf727785" class="numbered-list" start="1"><li>spark.sql(“select * from sql_table”)</li></ol><ol type="a" id="fec2b01d-abd0-4d7c-818f-c606a0229c1f" class="numbered-list" start="2"><li>The above is a transformation, not an action.</li></ol></li></ol><ol type="1" id="f5a74ba2-a4df-4042-8464-2344e31d412d" class="numbered-list" start="10"><li>Read a CSV file into pyspark<ol type="a" id="90fb7af5-16f3-4d84-9abb-0c4a08fd50e3" class="numbered-list" start="1"><li>spark.read.csv(‘file’, inferSchema= True, header = True)</li></ol></li></ol><ol type="1" id="010cac04-156f-42a5-bed5-b5f270cf804d" class="numbered-list" start="11"><li>Group by<ol type="a" id="6aca205f-6f8e-4672-a430-00f31c63bd2b" class="numbered-list" start="1"><li>df.groupby(‘city).sum(‘sales’).show()</li></ol></li></ol><ol type="1" id="27c5a333-31ae-4cfc-8cd5-3d80ee02aec7" class="numbered-list" start="12"><li>Create a dataframe from an RDD<ol type="a" id="54956af5-304c-4d72-ab8d-10b42eb01c47" class="numbered-list" start="1"><li>spark.createDataFrame()</li></ol></li></ol><ol type="1" id="62a8880a-a22c-4838-ba66-77a395822f00" class="numbered-list" start="13"><li>convert a string to datetime<ol type="a" id="e7c5cefa-338a-4bad-bb6c-d0605985369b" class="numbered-list" start="1"><li>df.to_datetime()</li></ol></li></ol><p id="241cc4df-937c-4448-a6a9-68332a445b46" class=""><strong>Broadcasting</strong></p><ol type="1" id="11692a5f-e1b3-4b34-842e-abca9730b66b" class="numbered-list" start="1"><li>Broadcast a file:<ol type="a" id="af31f3ae-7545-4442-9db4-aad187f566c6" class="numbered-list" start="1"><li>sc.broadcast(var)</li></ol></li></ol><p id="880b9b76-5bfe-4786-8879-fbf90371af6f" class=""><strong>DataBricks fs and dbutils</strong></p><ol type="1" id="3fdcb9e5-c535-43d9-8bca-200214a8ed4f" class="numbered-list" start="1"><li>%fs ls</li></ol><ol type="1" id="8b38589f-adce-44d4-b976-e30843d1cec9" class="numbered-list" start="2"><li>dbutils.fs.ls(&#x27;/FileStore/tables&#x27;)</li></ol><ol type="1" id="0f68d9e5-1d39-4991-ae31-83b35fa725f5" class="numbered-list" start="3"><li>dbutils.fs.head(&#x27;/FileStore/tables/sales_info.csv&#x27;)</li></ol><ol type="1" id="10548b52-2355-43da-9505-b261d42c9669" class="numbered-list" start="4"><li>db dbutils.fs.mkdirs(&quot;NewDir&quot;)utils.fs.help()</li></ol><ol type="1" id="196dd823-0653-4486-a0ef-cfcd68bb9e22" class="numbered-list" start="5"><li>dbutils.fs.cp(&quot;old_path&quot;, &quot;new_path&quot;)</li></ol><ol type="1" id="7d36e03b-45c0-4779-a7a7-b407f75251a5" class="numbered-list" start="6"><li>dbutils.fs.rm(&quot;/FileStore/tables/mydata/b1.csv&quot;)</li></ol><ol type="1" id="3ab0709c-2679-4f28-b27e-494eb7cbd0e6" class="numbered-list" start="7"><li>dbutils.notebook.help()</li></ol><ol type="1" id="cc15c718-d137-4deb-b645-1c57db061892" class="numbered-list" start="8"><li>dbutils.notebook.run(notebook, timeout, args)</li></ol><ol type="1" id="de92def4-20d6-4137-be50-1547633520a8" class="numbered-list" start="9"><li>dbutils.notebook.exit()</li></ol><ol type="1" id="888c2aad-93b5-4b02-a4c6-37fb2d21b88e" class="numbered-list" start="10"><li>dbutils.widgets.text(&quot;file_name&quot;, &quot;&quot;, &quot;&quot;)</li></ol><ol type="1" id="35c5bd2b-6365-49b8-b223-dcb44b70dfd3" class="numbered-list" start="11"><li>dbutils.widgets.removeAll()</li></ol><ol type="1" id="d7827e0d-d50a-4574-8ac0-3eb4150989c0" class="numbered-list" start="12"><li>folder_path = dbutils.widgets.get(&quot;file_name&quot;)</li></ol><p id="383dea92-6e5a-4c81-920b-61dceb0157c7" class=""><strong>DataBricks sql</strong></p><ol type="1" id="8d520851-32b5-48f7-8b86-b5e0feb5cc0e" class="numbered-list" start="1"><li>%sql select * from sales_csv limit 5;</li></ol><p id="9302a472-4c3a-4c43-a2a2-5cc564aea0b5" class=""><strong>DataBricks DataFrame</strong></p><ol type="1" id="02ed9302-4619-4bcc-b29b-a1170d2de7cb" class="numbered-list" start="1"><li>df4.union(df4).show()</li></ol><ol type="1" id="c5bd8183-3e94-4311-994c-03bb7008b3a4" class="numbered-list" start="2"><li>df4.unionAll(df4).show()</li></ol><ol type="1" id="feee7b24-0fed-445b-ba30-480f1714bc7f" class="numbered-list" start="3"><li>df4_union.drop_duplicates().show()</li></ol><ol type="1" id="989fcfe3-e08c-4c77-a8a1-12aafe53e11c" class="numbered-list" start="4"><li>df=spark.read.format(&quot;csv&quot;).option( &#x27;sep&#x27;,&quot;,&quot;).option(&#x27;header&#x27;,False).option(&#x27;mode&#x27;,&#x27;permissive&#x27;).schema(df_schema).option(“columnNameOfCorruptRecord”, “corrupt_record”).load(&quot;dbfs:/FileStore/tables/mydata/currupt_data.csv&quot;)</li></ol><ol type="1" id="2c751778-6581-4c9e-860a-31264cc5bf7f" class="numbered-list" start="5"><li>df.rdd.getNumPartitions()</li></ol><ol type="1" id="b4daf485-4e07-40a0-b1b5-319a40300ea3" class="numbered-list" start="6"><li></li></ol><p id="ae68b315-3c44-4d39-9ed5-eeb58b2851fc" class=""><strong>Modules</strong></p><ol type="1" id="fa026ab5-4f67-4b5a-9947-66d62a95116d" class="numbered-list" start="1"><li>from pyspark import SparkContext</li></ol><ol type="1" id="ec4312a6-e215-40b2-8115-3e4afb4db3c6" class="numbered-list" start="2"><li>from pyspark.sql import SparkSession<ol type="a" id="d95ed171-1c99-46b7-8fc8-12ec5eaf91e3" class="numbered-list" start="1"><li>SparkSession.builder.appname(‘name’).getOrCreate()</li></ol></li></ol><ol type="1" id="0c4c9120-480b-49c0-a52c-9d84843373f3" class="numbered-list" start="3"><li>from pyspark.sql.types import StructType, StructField</li></ol><ol type="1" id="6cf3c275-1c22-4c88-afe6-5b837883fff4" class="numbered-list" start="4"><li>import pyspark.sql.functions as f</li></ol></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>